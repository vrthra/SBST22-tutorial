{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject Registry\n",
    "\n",
    "We store all our subject programs in `program_src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_src = {\n",
    "    'calculator.py': utils.slurp('subjects/calculator.py'),\n",
    "    'microjson.py': utils.slurp('subjects/microjson.py')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Mangers\n",
    "\n",
    "The context managers are probes inserted into the source code so that we know when execution enters and exits specific control flow structures such as conditionals and loops. Note that source code for these probes are not really a requirement. They can be inserted directly on binaries too, or even dynamically inserted using tools such as `dtrace`. For now, we make our life simple using AST editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build/config.py\n",
    "import urllib.parse # used for encoding meta chars.\n",
    "# This is useful for parsers such as PEG where the argument\n",
    "# is important as the name of the non-terminal.\n",
    "ENCODE_ARGS = False\n",
    "def encode_method_name(name, my_args):\n",
    "    if not ENCODE_ARGS: return name\n",
    "    if not my_args: return name\n",
    "    # trick to convert args that are not of type str for later.\n",
    "    if hasattr(my_args[0], 'tag'):\n",
    "        name = \"%s:%s\" % (my_args[0].tag, name)\n",
    "    else:\n",
    "        return \"%s(%s)\" % (name, urllib.parse.quote('_'.join([str(i) for i in my_args])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method context\n",
    "The `method__` context handles the assignment of method name, as well as storing the method stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build/mimid_context.py\n",
    "import ipynb.fs.full.x1_1_TrackAccess as taints\n",
    "import config\n",
    "def to_key(method, name, num):\n",
    "    return '%s:%s_%s' % (method, name, num)\n",
    "\n",
    "class method__:\n",
    "    def __init__(self, name, args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.name = config.encode_method_name(name, args)\n",
    "        self.method_name = self.name\n",
    "        taints.trace_call(self.name)\n",
    "\n",
    "    def __enter__(self):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self._old_name =  taints.trace_set_method(self.name)\n",
    "        self.stack = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        taints.trace_return()\n",
    "        taints.trace_set_method(self._old_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stack context stores the current prefix and handles updating the stack that is stored at the method context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a build/mimid_context.py\n",
    "class stack__:\n",
    "    def __init__(self, name, num, method_i, can_empty):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.stack = method_i.stack\n",
    "        self.method_name = method_i.method_name\n",
    "        self.can_empty = can_empty # * means yes. + means no, ? means to be determined\n",
    "        self.name, self.num = name, num\n",
    "\n",
    "    def __enter__(self):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        if self.name in {'while'}:\n",
    "            self.stack.append('loop_%d' % self.num)\n",
    "        elif self.name in {'if'}:\n",
    "            self.stack.append('if_%d' % self.num)\n",
    "        else:\n",
    "            assert False\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.stack.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope context\n",
    "The scope context correctly identifies when the control structure is entered into, and exited (in case of loops) and the alternative entered into (in case of if conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a build/mimid_context.py\n",
    "import json\n",
    "\n",
    "def encode_name(method, ctrl, ctrl_id, alt_num, can_empty, stack):\n",
    "    assert ctrl in {'while', 'if'}\n",
    "    return '%s:%s_%s,%s %s#%s' % (method, ctrl, ctrl_id, alt_num, can_empty, json.dumps(stack))\n",
    "\n",
    "class scope__:\n",
    "    def __init__(self, alt, stack_i, method_i):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        self.name, self.num, self.method, self.alt = stack_i.name, stack_i.num, stack_i.method_name, alt\n",
    "        self.stack = stack_i.stack\n",
    "        self.method_name = stack_i.method_name\n",
    "        self.can_empty = stack_i.can_empty\n",
    "\n",
    "    def __enter__(self):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        if self.name in {'while'}:\n",
    "            # we do not have to keep track of iteration id. We\n",
    "            # instead recognize different parts of loops by compatible patterns\n",
    "            pass\n",
    "            #self.stack[-1] += 1\n",
    "        elif self.name in {'if'}:\n",
    "            pass\n",
    "        else:\n",
    "            assert False, self.name\n",
    "        encoded_name = encode_name(self.method, self.name, self.num, self.alt, self.can_empty, self.stack)\n",
    "        taints.trace_call(encoded_name)\n",
    "        self._old_name = taints.trace_set_method(self.name)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if not taints.METHOD_NUM_STACK: return\n",
    "        taints.trace_return()\n",
    "        taints.trace_set_method(self._old_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting the source to track control flow and taints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rewrite the methods so that method bodies are enclosed in a `method__` context manager, any `if`conditions and `while` loops (only `while` for now) are enclosed in an outer `stack__` and inner `scope__` context manager. This lets us track when the corresponding scopes are entered into and left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriter\n",
    "\n",
    "The `Rewriter` class handles inserting tracing probes into methods and control structures. Essentially, we insert a `with` scope for the method body, and a `with` scope outside both `while` and `if` scopes. Finally, we insert a `with` scope inside the `while` and `if` scopes. IMPORTANT: We only implement the `while` context. Similar should be implemented for the `for` context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The method context wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few counters to provide unique identifiers for context managers. Essentially, we number each if and while that we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(ast.NodeTransformer):\n",
    "    def init_counters(self):\n",
    "        self.if_counter = 0\n",
    "        self.while_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `methods[]` array is used to keep track of the current method stack during execution. Epsilon and NoEpsilon are simply constants that I use to indicate whether an IF or a Loop is nullable or not. If it is nullable, I mark it with Epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "Epsilon = '-'\n",
    "NoEpsilon = '='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `wrap_in_method()` generates a wrapper for method definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def wrap_in_method(self, body, args):\n",
    "        method_name_expr = ast.Str(methods[-1])\n",
    "        my_args = ast.List(args.args, ast.Load())\n",
    "        args = [method_name_expr, my_args]\n",
    "        scope_expr = ast.Call(func=ast.Name(id='method__', ctx=ast.Load()), args=args, keywords=[])\n",
    "        return [ast.With(items=[ast.withitem(scope_expr, ast.Name(id='_method__'))], body=body)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `visit_FunctionDef()` is the method rewriter that actually does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def visit_FunctionDef(self, tree_node):\n",
    "        self.init_counters()\n",
    "        methods.append(tree_node.name)\n",
    "        self.generic_visit(tree_node)\n",
    "        tree_node.body = self.wrap_in_method(tree_node.body, tree_node.args)\n",
    "        return tree_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rewrite_def()` method wraps the function definitions in scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_def(src):\n",
    "    v = ast.fix_missing_locations(Rewriter().visit(ast.parse(src)))\n",
    "    return ast.unparse(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in enumerate(program_src['calculator.py'].split('\\n')):\n",
    "    print(i, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rewrite_def('\\n'.join(program_src['calculator.py'].split('\\n')[13:19])), 'calculator.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The stack wrapper\n",
    "\n",
    "The method `wrap_in_outer()` adds a `with ..stack..()` context _outside_ the control structures. The stack is used to keep track of the current control structure stack for any character comparison made. Notice the `can_empty` parameter. This indicates that the particular structure is _nullable_. For `if` we can make the condition right away. For `while` we postpone the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def wrap_in_outer(self, name, can_empty, counter, node):\n",
    "        name_expr = ast.Str(name)\n",
    "        can_empty_expr = ast.Str(can_empty)\n",
    "        counter_expr = ast.Num(counter)\n",
    "        method_id = ast.Name(id='_method__')\n",
    "        args = [name_expr, counter_expr, method_id, can_empty_expr]\n",
    "        scope_expr = ast.Call(func=ast.Name(id='stack__', ctx=ast.Load()),\n",
    "                args=args, keywords=[])\n",
    "        return ast.With(\n",
    "            items=[ast.withitem(scope_expr, ast.Name(id='%s_%d_stack__' % (name, counter)))], \n",
    "            body=[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The scope wrapper\n",
    "The method `wrap_in_inner()` adds a `with ...scope..()` context immediately inside the control structure. For `while`, this means simply adding one `with ...scope..()` just before the first line. For `if`, this means adding one `with ...scope...()` each to each branch of the `if` condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def wrap_in_inner(self, name, counter, val, body):\n",
    "        val_expr = ast.Num(val)\n",
    "        stack_iter = ast.Name(id='%s_%d_stack__' % (name, counter))\n",
    "        method_id = ast.Name(id='_method__')\n",
    "        args = [val_expr, stack_iter, method_id]\n",
    "        scope_expr = ast.Call(func=ast.Name(id='scope__', ctx=ast.Load()),\n",
    "                args=args, keywords=[])\n",
    "        return [ast.With(\n",
    "            items=[ast.withitem(scope_expr, ast.Name(id='%s_%d_%d_scope__' % (name, counter, val)))], \n",
    "            body=body)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewriting `If` conditions\n",
    "\n",
    "While rewriting if conditions, we have to take care of the cascading if conditions (`elsif`), which is represented as nested if conditions in AST. They do not require separate `stack` context, but only separate `scope` contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def process_if(self, tree_node, counter, val=None):\n",
    "        if val is None: val = 0\n",
    "        else: val += 1\n",
    "        if_body = []\n",
    "        self.generic_visit(tree_node.test)\n",
    "        for node in tree_node.body:\n",
    "            if_body.append(self.generic_visit(ast.Module(node)).body)\n",
    "        tree_node.body = self.wrap_in_inner('if', counter, val, if_body)\n",
    "\n",
    "        # else part.\n",
    "        if len(tree_node.orelse) == 1 and isinstance(tree_node.orelse[0], ast.If):\n",
    "            self.process_if(tree_node.orelse[0], counter, val)\n",
    "        else:\n",
    "            if tree_node.orelse:\n",
    "                val += 1\n",
    "                for node in tree_node.orelse: self.generic_visit(node)\n",
    "                tree_node.orelse = self.wrap_in_inner('if', counter, val, tree_node.orelse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While rewriting `if` conditions, we have to take care of the cascading `if` conditions, which is represented as nested `if` conditions in AST. We need to identify whether the cascading `if` conditions (`elsif`) have an empty `orelse` clause or not. If it has an empty `orelse`, then the entire set of `if` conditions may be excised, and still produce a valid value. Hence, it should be marked as optional. The `visit_If()` checks if the cascading `ifs` have an `orelse` or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def visit_If(self, tree_node):\n",
    "        self.if_counter += 1\n",
    "        counter = self.if_counter\n",
    "        #is it empty\n",
    "        start = tree_node\n",
    "        while start:\n",
    "            if isinstance(start, ast.If):\n",
    "                if not start.orelse:\n",
    "                    start = None\n",
    "                elif len(start.orelse) == 1:\n",
    "                    start = start.orelse[0]\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        self.process_if(tree_node, counter=self.if_counter)\n",
    "        can_empty = NoEpsilon if start else Epsilon  # NoEpsilon for + and Epsilon for *\n",
    "        return self.wrap_in_outer('if', can_empty, counter, tree_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewriting `while` loops\n",
    "\n",
    "Rewriting while loops are simple. We wrap them in `stack` and `scope` contexts. We do not implement the `orelse` feature yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewriter(Rewriter):\n",
    "    def visit_While(self, tree_node):\n",
    "        self.generic_visit(tree_node)\n",
    "        self.while_counter += 1\n",
    "        counter = self.while_counter\n",
    "        test = tree_node.test\n",
    "        body = tree_node.body\n",
    "        assert not tree_node.orelse\n",
    "        tree_node.body = self.wrap_in_inner('while', counter, 0, body)\n",
    "        return self.wrap_in_outer('while', '?', counter, tree_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_cf(src, original):\n",
    "    v = ast.fix_missing_locations(Rewriter().visit(ast.parse(src)))\n",
    "    return ast.unparse(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example with `if` conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(program_src['calculator.py'].split('\\n')[13:19]), 'calculator.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rewrite_cf('\\n'.join(program_src['calculator.py'].split('\\n')[13:19]), 'calculator.py').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example with `while` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(program_src['calculator.py'].split('\\n')[6:12]), 'calculator.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rewrite_cf('\\n'.join(program_src['calculator.py'].split('\\n')[6:12]), 'calculator.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the complete instrumented source\n",
    "\n",
    "For the complete instrumented source, we need to first make sure that all necessary imports are satisfied. Next, we also need to invoke the parser with the necessary tainted input and output the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.x1_1_TrackAccess import InRewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(src, original):\n",
    "    src = ast.fix_missing_locations(InRewriter().visit(ast.parse(src)))\n",
    "    v = ast.fix_missing_locations(Rewriter().visit(ast.parse(src)))\n",
    "    header = \"\"\"\n",
    "from mimid_context import scope__, stack__, method__\n",
    "import json\n",
    "import sys\n",
    "import ipynb.fs.full.x1_1_TrackAccess as taints\n",
    "from ipynb.fs.full.x1_1_TrackAccess import taint_wrap__\n",
    "    \"\"\"\n",
    "    source = ast.unparse(v)\n",
    "    footer = \"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    js = []\n",
    "    for arg in sys.argv[1:]:\n",
    "        with open(arg) as f:\n",
    "            mystring = f.read().strip().replace('\\\\n', ' ')\n",
    "        taints.trace_init()\n",
    "        tainted_input = taints.wrap_input(mystring)\n",
    "        main(tainted_input)\n",
    "        assert tainted_input.comparisons\n",
    "        j = {\n",
    "        'comparisons_fmt': 'idx, char, method_call_id',\n",
    "        'comparisons':taints.convert_comparisons(tainted_input.comparisons, mystring),\n",
    "        'method_map_fmt': 'method_call_id, method_name, children',\n",
    "        'method_map': taints.convert_method_map(taints.METHOD_MAP),\n",
    "        'inputstr': mystring,\n",
    "        'original': %s,\n",
    "        'arg': arg}\n",
    "        js.append(j)\n",
    "    print(json.dumps(js))\n",
    "\"\"\"\n",
    "    footer = footer % repr(original)\n",
    "    return \"%s\\n%s\\n%s\" % (header, source, footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_parse_rewritten = rewrite(program_src['calculator.py'], original='calculator.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_parse_rewritten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Transformed Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write the transformed sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in program_src:\n",
    "    print(file_name)\n",
    "    with open(\"build/%s\" % file_name, 'w+') as f:\n",
    "        f.write(rewrite(program_src[file_name], file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one can generate traces for the `calc` program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.do(['mkdir','-p','samples/calc']).returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samples/calc/0.csv', 'w+') as f:\n",
    "    print('9-(16+72)*3/458', file=f)\n",
    "    \n",
    "with open('samples/calc/1.csv', 'w+') as f:\n",
    "    print('(9)+3/4/58', file=f)\n",
    "    \n",
    "with open('samples/calc/2.csv', 'w+') as f:\n",
    "    print('8*3/40', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_trace_out = utils.do(\"env PYTHONPATH='.:src:subjects' %s build/calculator.py samples/calc/*.csv\" % sys.executable, shell=True).stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_trace = json.loads(calc_trace_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct in calc_trace:\n",
    "    cstr = ct['inputstr']\n",
    "    print(cstr)\n",
    "    seen = set()\n",
    "    for ci, char, m in ct['comparisons']:\n",
    "        assert(char == cstr[ci])\n",
    "        seen.add(ci)\n",
    "        print(ci, cstr[ci])\n",
    "    print(seen, cstr)\n",
    "    assert len(seen) == len(cstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining the Traces Generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the Method Tree with Attached Character Comparisons\n",
    "\n",
    "Reconstruct the actual method trace from a trace with the following\n",
    "format\n",
    "```\n",
    "key   : [ mid, method_name, children_ids ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_method_tree(method_map):\n",
    "    first_id = None\n",
    "    tree_map = {}\n",
    "    for key in method_map:\n",
    "        m_id, m_name, m_children = method_map[key]\n",
    "        children = []\n",
    "        if m_id in tree_map:\n",
    "            # just update the name and children\n",
    "            assert not tree_map[m_id]\n",
    "            tree_map[m_id]['id'] = m_id\n",
    "            tree_map[m_id]['name'] = m_name\n",
    "            tree_map[m_id]['indexes'] = []\n",
    "            tree_map[m_id]['children'] = children\n",
    "        else:\n",
    "            assert first_id is None\n",
    "            tree_map[m_id] = {'id': m_id, 'name': m_name, 'children': children, 'indexes': []}\n",
    "            first_id = m_id\n",
    "\n",
    "        for c in m_children:\n",
    "            assert c not in tree_map\n",
    "            val = {}\n",
    "            tree_map[c] = val\n",
    "            children.append(val)\n",
    "    return first_id, tree_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it. The first element in the returned tuple is the id of the bottom most method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, calc_method_tree1 = reconstruct_method_tree(calc_trace[0]['method_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_node(node):\n",
    "    symbol = str(node['id'])\n",
    "    annotation = str(node['name'])\n",
    "    return \"<%s:%s>\" % (symbol, annotation)\n",
    "\n",
    "def get_children(node):\n",
    "    return node['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayTree(utils.DisplayTree):\n",
    "    def extract_node(self, node, id):\n",
    "        symbol = str(node['id'])\n",
    "        children = node['children']\n",
    "        annotation = str(node['name'])\n",
    "        return \"<%s:%s>\" % (symbol, annotation), children, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DisplayTree(calc_method_tree1[0]).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(v):\n",
    "    return Image(v.render(format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " zoom(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying last comparisons\n",
    "We need only the last comparisons made on any index. This means that we should care for only the last parse in an ambiguous parse. So, we assign the method that last touched an index to be its consumer.\n",
    "\n",
    "However, to make concessions for real world, we also check if we are overwriting a child (`HEURISTIC`). Essentially, if the heursitic is enabled, then if the current method id (`midP`) is smaller than the `midC` already stored in the last comparison map, then it means that `midP` is a parent that called `midC` previously, and now accessing an index that `midC` touched. This happens when the parent tries to find a substring like `#` in the entirety of the original string. (Note that we have seen this only in `URLParser`). (Note that this heuristic does not restrict reparsing by another function call -- in such a case, `midC` will not smaller than `midP`). So, perhaps, we should let the child keep the ownership. However, there is one more wrinkle. If the character being contested was the last index touched by our `mid`, then it is likely that it was simply a boundary check. In that case, we should let the parent own this character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_COMPARISON_HEURISTIC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_comparisons(comparisons):\n",
    "    last_cmp_only = {}\n",
    "    last_idx = {}\n",
    "\n",
    "    # get the last indexes compared in methods.\n",
    "    for idx, char, mid in comparisons:\n",
    "        if mid in last_idx:\n",
    "            if idx > last_idx[mid]:\n",
    "                last_idx[mid] = idx\n",
    "        else:\n",
    "            last_idx[mid] = idx\n",
    "\n",
    "    for idx, char, mid in comparisons:\n",
    "        if LAST_COMPARISON_HEURISTIC:\n",
    "            if idx in last_cmp_only:\n",
    "                midC = last_cmp_only[idx]\n",
    "                if midC > mid:\n",
    "                    # midC is a child of mid.\n",
    "                    # do not clobber children unless it was the last character\n",
    "                    # for that child.\n",
    "                    if last_idx[mid] == idx:\n",
    "                        # if it was the last index, may be the child used it\n",
    "                        # as a boundary check.\n",
    "                        pass\n",
    "                    else:\n",
    "                        # do not overwrite the current value of `last_cmp_only[idx]`\n",
    "                        continue\n",
    "        last_cmp_only[idx] = mid\n",
    "    return last_cmp_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_last_comparisons1 = last_comparisons(calc_trace[0]['comparisons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_last_comparisons1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attaching characters to the tree\n",
    "Add the comparison indexes to the method tree that we constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_comparisons(method_tree, comparisons):\n",
    "    for idx in comparisons:\n",
    "        mid = comparisons[idx]\n",
    "        method_tree[mid]['indexes'].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it. Note which method call each input index is associated. For example, the first index is associated with method call id: 6, which corresponds to `is_digit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_comparisons(calc_method_tree1, calc_last_comparisons1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayTree2(DisplayTree):\n",
    "    def __init__(self, derivation_tree, istr):\n",
    "        super().__init__(derivation_tree)\n",
    "        self.istr = istr\n",
    "\n",
    "    def extract_node(self, node, id):\n",
    "        symbol = str(node['id'])\n",
    "        children = node['children']\n",
    "        annotation = str(node['name'])\n",
    "        indexes = repr(tuple([self.istr[i] for i in node['indexes']]))\n",
    "        return \"<%s %s>\" % (annotation, indexes), children, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(v:=DisplayTree2(calc_method_tree1[0], calc_trace[0]['inputstr']).display())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `to_node()` a convenience function that, given a list of _contiguous_ indexes and original string, translates it to a leaf node of a tree (that corresponds to the derivation tree syntax in the Fuzzingbook) with a string, empty children, and starting node and ending node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a list of indexes to a corresponding terminal tree node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_node(idxes, my_str):\n",
    "    assert len(idxes) == idxes[-1] - idxes[0] + 1\n",
    "    assert min(idxes) == idxes[0]\n",
    "    assert max(idxes) == idxes[-1]\n",
    "    return my_str[idxes[0]:idxes[-1] + 1], [], idxes[0], idxes[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one would use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in calc_method_tree1.keys():\n",
    "    idxs = calc_method_tree1[k]['indexes']\n",
    "    if idxs:\n",
    "        print(k, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_node(calc_method_tree1[9]['indexes'], calc_trace[0]['inputstr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to identify the terminal (leaf) nodes. For that, we want to group contiguous letters in a node together, and call it a leaf node. So, convert our list of indexes to lists of contiguous indexes first, then convert them to terminal tree nodes. Then, return a set of one level child nodes with contiguous chars from indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_to_children(indexes, my_str):\n",
    "    lst = [\n",
    "        list(map(itemgetter(1), g))\n",
    "        for k, g in it.groupby(enumerate(indexes), lambda x: x[0] - x[1])\n",
    "    ]\n",
    "\n",
    "    return [to_node(n, my_str) for n in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_children(calc_method_tree1[9]['indexes'], calc_trace[0]['inputstr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to remove the overlap from the trees we have so far. The idea is that, given a node, each child node of that node should be uniquely responsible for a specified range of characters, with no overlap allowed between the children. The starting of the first child to ending of the last child will be the range of the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Overlap\n",
    "If overlap is found, the tie is biased to the later child. That is, the later child gets to keep the range, and the former child is recursively traversed to remove overlaps from its children. If a child is completely included in the overlap, the child is excised. A few convenience functions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_item_overlap(r, r_):\n",
    "    (s, e), (s_, e_) = r, r_\n",
    "    return ((s_ >= s and s_ <= e) or \n",
    "            (e_ >= s and e_ <= e) or \n",
    "            (s_ <= s and e_ >= e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_second_item_included(r, r_):\n",
    "    (s, e), (s_, e_) = r, r_\n",
    "    return (s_ >= s and e_ <= e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_overlap(ranges, r_):\n",
    "    return {r for r in ranges if does_item_overlap(r, r_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_included(ranges, r_):\n",
    "    return {r for r in ranges if is_second_item_included(r, r_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlap_from(original_node, orange):\n",
    "    node, children, start, end = original_node\n",
    "    new_children = []\n",
    "    if not children:\n",
    "        return None\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for child in children:\n",
    "        if does_item_overlap(child[2:4], orange):\n",
    "            new_child = remove_overlap_from(child, orange)\n",
    "            if new_child: # and new_child[1]:\n",
    "                if start == -1: start = new_child[2]\n",
    "                new_children.append(new_child)\n",
    "                end = new_child[3]\n",
    "        else:\n",
    "            new_children.append(child)\n",
    "            if start == -1: start = child[2]\n",
    "            end = child[3]\n",
    "    if not new_children:\n",
    "        return None\n",
    "    assert start != -1\n",
    "    assert end != -1\n",
    "    return (node, new_children, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there is no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_overlap(arr):\n",
    "    my_ranges = {}\n",
    "    for a in arr:\n",
    "        _, _, s, e = a\n",
    "        r = (s, e)\n",
    "        included = is_included(my_ranges, r)\n",
    "        if included:\n",
    "            continue  # we will fill up the blanks later.\n",
    "        else:\n",
    "            overlaps = has_overlap(my_ranges, r) \n",
    "            if overlaps:\n",
    "                # unlike include which can happen only once in a set of\n",
    "                # non-overlapping ranges, overlaps can happen on multiple parts.\n",
    "                # The rule is, the later child gets the say. So, we recursively\n",
    "                # remove any ranges that overlap with the current one from the\n",
    "                # overlapped range.\n",
    "                assert len(overlaps) == 1\n",
    "                oitem = list(overlaps)[0]\n",
    "                v = remove_overlap_from(my_ranges[oitem], r)\n",
    "                del my_ranges[oitem]\n",
    "                if v:\n",
    "                    my_ranges[v[2:4]] = v\n",
    "                my_ranges[r] = a\n",
    "            else:\n",
    "                my_ranges[r] = a\n",
    "    res = my_ranges.values()\n",
    "    # assert no overlap, and order by starting index\n",
    "    s = sorted(res, key=lambda x: x[2])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate derivation tree\n",
    "\n",
    "Convert a mapped tree to the _fuzzingbook_ style derivation tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(node, my_str):\n",
    "    method_name = (\"<%s>\" % node['name']) if node['name'] is not None else '<START>'\n",
    "    indexes = node['indexes']\n",
    "    node_children = [to_tree(c, my_str) for c in node.get('children', [])]\n",
    "    idx_children = indexes_to_children(indexes, my_str)\n",
    "    children = no_overlap([c for c in node_children if c is not None] + idx_children)\n",
    "    if not children:\n",
    "        return None\n",
    "    start_idx = children[0][2]\n",
    "    end_idx = children[-1][3]\n",
    "    si = start_idx\n",
    "    my_children = []\n",
    "    # FILL IN chars that we did not compare. This is likely due to an i + n\n",
    "    # instruction.\n",
    "    for c in children:\n",
    "        if c[2] != si:\n",
    "            sbs = my_str[si: c[2]]\n",
    "            my_children.append((sbs, [], si, c[2] - 1))\n",
    "        my_children.append(c)\n",
    "        si = c[3] + 1\n",
    "\n",
    "    m = (method_name, my_children, start_idx, end_idx)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(v:=utils.display_tree(to_tree(calc_method_tree1[0], calc_trace[0]['inputstr'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Complete Miner\n",
    "\n",
    "We now put everything together. The `miner()` takes the traces, produces trees out of them, and verifies that the trees actually correspond to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_str(tree): # Non recursive\n",
    "    expanded = []\n",
    "    to_expand = [tree]\n",
    "    while to_expand:\n",
    "        (key, children, *rest), *to_expand = to_expand\n",
    "        if utils.is_nt(key):\n",
    "            to_expand = children + to_expand\n",
    "        else:\n",
    "            assert not children\n",
    "            expanded.append(key)\n",
    "    return ''.join(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miner(call_traces):\n",
    "    my_trees = []\n",
    "    for call_trace in call_traces:\n",
    "        with open('last_trace.json', 'w+') as f:\n",
    "            json.dump([call_trace], fp=f)\n",
    "        method_map = call_trace['method_map']\n",
    "\n",
    "        first, method_tree = reconstruct_method_tree(method_map)\n",
    "        comparisons = call_trace['comparisons']\n",
    "        attach_comparisons(method_tree, last_comparisons(comparisons))\n",
    "\n",
    "        my_str = call_trace['inputstr']\n",
    "\n",
    "        tree = to_tree(method_tree[first], my_str)\n",
    "        my_tree = {'tree': tree, 'original': call_trace['original'], 'arg': call_trace['arg']}\n",
    "        assert tree_to_str(tree) == my_str\n",
    "        my_trees.append(my_tree)\n",
    "    return my_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `miner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_calc_trees = miner(calc_trace)\n",
    "calc_tree = mined_calc_trees[0]\n",
    "zoom(v:=utils.display_tree(calc_tree['tree']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalize Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants to limit our exploration. The `MAX_PROC_SAMPLES` is used to specify how many samples to match for getting a swap patter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PROC_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching and book keeping variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_MAP = {}\n",
    "NODE_REGISTER = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_generalizer():\n",
    "    global EXEC_MAP\n",
    "    global NODE_REGISTER\n",
    "    EXEC_MAP.clear()\n",
    "    NODE_REGISTER.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are evaluating Python functions, we need a wrapper to make them executable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small library function to convert from tuple to lists so that we can modify a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_modifiable(derivation_tree):\n",
    "    node, children, *rest = derivation_tree\n",
    "    return [node, [to_modifiable(c) for c in children], *rest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is that, sometimes one finds that our central assumption -- that a fragment consumed by a function can be replaced by another fragment consumed by the same function elsewhere -- doesn't hold. This can be seen in functions that take an additional argument to specify what it should match. In such cases, we want to try and find out how to distinguish between these function invocations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`node_include()` is a library function that checks whether the node `j` is within the boundary of `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_include(i, j):\n",
    "    name_i, children_i, s_i, e_i = i\n",
    "    name_j, children_j, s_j, e_j = j\n",
    "    return is_second_item_included((s_i, e_i), (s_j, e_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_ref()` takes a `node` datastructure and searches for `node_name`. It returns the first instance found. This allows us to easily swap nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref(node, node_name):\n",
    "    name, children, *rest = node\n",
    "    if name == node_name:\n",
    "        return node\n",
    "    for child in children:\n",
    "        res = get_ref(child, node_name)\n",
    "        if res is not None: return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `replace_nodes()` function try to replace the contents of the first node with the _contents_ of the second (That is, the tree that has these nodes will automatically be modified), collect the produced string from the tree, and reset any changes. The arguments are tuples with the following format: (node, file_name, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the given node in a2 by the node in a1\n",
    "def replace_nodes(a2, a1):\n",
    "    node2, _, t2 = a2\n",
    "    node1, _, t1 = a1\n",
    "    str2_old = tree_to_str(t2)\n",
    "\n",
    "    # first change the name of the node, then copy the tree.\n",
    "    tmpl_name = '___cmimid___'\n",
    "    old_name = node2[0]\n",
    "    node2[0] = tmpl_name\n",
    "    t2_new = utils.deep_copy(t2)\n",
    "    node2[0] = old_name\n",
    "\n",
    "    # now find the reference to tmpl_name in t2_new\n",
    "    node2 = get_ref(t2_new, tmpl_name)\n",
    "    node2.clear()\n",
    "    for n in node1:\n",
    "        node2.append(n)\n",
    "    str2_new = tree_to_str(t2_new)\n",
    "    assert str2_old != str2_new\n",
    "    return t2_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can a given node be replaced with another? The idea is, given two nodes (possibly from two trees), can the first node be replaced by the second, and still result in a valid string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_replaceable_with_b(a1, a2, module):\n",
    "    n1, f1, t1 = a1\n",
    "    n2, f2, t2 = a2\n",
    "    if tree_to_str(n1) == tree_to_str(n2): return True\n",
    "    t_x = replace_nodes(a1, (('XXXX', []), None, t2))\n",
    "    x = tree_to_str(t_x)\n",
    "    updated_tree = replace_nodes(a1, a2)\n",
    "    updated_string = tree_to_str(updated_tree)\n",
    "    o = tree_to_str(t1)\n",
    "    v = check(o, x, n1[0], updated_tree, module, tree_to_str(a1[0]), tree_to_str(a2[0]))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_compatible(a1, a2, module):\n",
    "    t1 = is_a_replaceable_with_b(a1, a2, module)\n",
    "    if not t1: return False\n",
    "    t2 = is_a_replaceable_with_b(a2, a1, module)\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are fundamentally, two kinds of nodes. The first kind of node is a method node, that correspond to a method call. The second is a node that corresponds to a pseudo-method -- that is, a node that represents a loop or a conditional. Below are the predicates that identify such methods, parses, and reconstructs such nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_method(node):\n",
    "    node_name = node[0]\n",
    "    if (node_name[0], node_name[-1]) != ('<', '>'): return False\n",
    "    return not is_node_pseudo(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_pseudo(node):\n",
    "    node_name = node[0]\n",
    "    if (node_name[0], node_name[-1]) != ('<', '>'): return False\n",
    "    if ':if_' in node_name: return True\n",
    "    if ':while_' in node_name: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pseudo_name(node_name):\n",
    "    assert (node_name[0], node_name[-1]) == ('<','>')\n",
    "    return decode_name(node_name[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_method_name(mname):\n",
    "    assert (mname[0], mname[-1]) == ('<', '>')\n",
    "    name = mname[1:-1]\n",
    "    if '.' in name:\n",
    "        nname, my_id = name.split('.')\n",
    "        return nname, my_id\n",
    "    else:\n",
    "        return name, '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_name(node_name_stack):\n",
    "    node_name, mstack = node_name_stack.split('#')\n",
    "    method_stack = json.loads(mstack)\n",
    "    method_ctrl_alt_name, can_empty = node_name.split(' ')\n",
    "    method, ctrl_cid_altid = method_ctrl_alt_name.split(':')\n",
    "    ctrl, cid_altid = ctrl_cid_altid.split('_')\n",
    "    assert ctrl in {'while', 'if'}\n",
    "    cid, altid = cid_altid.split(',')\n",
    "\n",
    "    if 'while' == ctrl:\n",
    "        assert altid == '0'\n",
    "    return method, ctrl, int(cid), altid, can_empty, method_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unparse_pseudo_name(method, ctrl, ctrl_id, alt_num, can_empty, cstack):\n",
    "    return \"<%s>\" % encode_name(method, ctrl, ctrl_id, alt_num, can_empty, cstack)\n",
    "\n",
    "def unparse_method_name(mname, my_id):\n",
    "    return '<%s.%s>' % (mname, my_id)\n",
    "\n",
    "def encode_name(method, ctrl, ctrl_id, alt_num, can_empty, stack):\n",
    "    assert ctrl in {'while', 'if'}\n",
    "    return '%s:%s_%s,%s %s#%s' % (method, ctrl, ctrl_id, alt_num, can_empty, json.dumps(stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `check()` function invokes the given subject call with the previously defined `check.py` wrapper, logs and returns the result of the call.\n",
    "\n",
    "**TODO**: What we really want to do, is to generate a new updated tree first after doing the tree surgery. Then, convert this tree to a parenthesized tree by simply doing `tree_to_string` with additional `{}` (or other open/close symbols that does not conflict with the input) attached when joining the nodes. That is,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_pstr(tree, op_='', _cl=''):\n",
    "    expanded = []\n",
    "    to_expand = [tree]\n",
    "    while to_expand:\n",
    "        (key, children, *_), *to_expand = to_expand\n",
    "        if utils.is_nt(key):\n",
    "            expanded.append(op_)\n",
    "            to_expand = children + [(_cl, [])] + to_expand\n",
    "        else:\n",
    "            assert not children\n",
    "            expanded.append(key)\n",
    "    return ''.join(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, when using `tree_to_string(my_tree, '{', '}')`, We will get a string that represents how the original string was parsed. For example `1+2+3` may be represented as `{{1+2}+3}`.\n",
    "\n",
    "Next, we need to run the non-parenthesized string resulting from the tree surgery through the program, and collect the resulting tree. Again, convert this tree to the parentesized version, and compare equality.\n",
    "\n",
    "With this, we can ensure that our tree nodes are correctly compatible, and secondly, we can ignore the return code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(o, x, e, ut, module, sa1, sa2):\n",
    "    s = tree_to_str(ut)\n",
    "    if s in EXEC_MAP: return EXEC_MAP[s]\n",
    "    updated_ps = tree_to_pstr(ut, op_='{', _cl='}')\n",
    "    tn = \"build/_test.csv\"\n",
    "    with open(tn, 'w+') as f: print(s, file=f)\n",
    "\n",
    "    trace_out = utils.do([sys.executable,\"build/%(m)s\" % {'m': module}, tn] ).stdout\n",
    "    val = None\n",
    "    v = False\n",
    "    parsed_ps = None\n",
    "    try:\n",
    "        val = json.loads(trace_out)\n",
    "        parsed_tree = miner(val)[0]['tree']\n",
    "        parsed_ps = tree_to_pstr(parsed_tree, op_='{', _cl='}')\n",
    "        v = (parsed_ps == updated_ps)\n",
    "    except:\n",
    "        parsed_ps = 'ERROR'\n",
    "        v = False\n",
    "  \n",
    "    with open('%s.log' % module, 'a+') as f:\n",
    "        print('------------------', file=f)\n",
    "        print(' '.join([sys.executable, \"build/%s\" % module, s]), file=f)\n",
    "        print('Checking:',e, file=f)\n",
    "        print('original:', repr(o), file=f)\n",
    "        print('tmpl:', repr(x), file=f)\n",
    "        print('updated:', repr(s), file=f)\n",
    "        print('XXXX:', repr(sa1), file=f)\n",
    "        print('REPL:', repr(sa2), file=f)\n",
    "        print('ops:', repr(updated_ps), file=f)\n",
    "        print('pps:', repr(parsed_ps), file=f)\n",
    "        print(\":=\", v, file=f)\n",
    "    #     print(' '.join([module, repr(s)]), file=f)\n",
    "    #     print(\"\\n\", file=f)\n",
    "    # v = (result.returncode == 0)\n",
    "    EXEC_MAP[s] = v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to collect all nodes of a particular kind together. `register_node()` correctly saves specific kinds of nodes separately as copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_node(node, tree, executable, input_file):\n",
    "    # we want to save a copy of the tree so we can modify it later. \n",
    "    node_name = node[0]\n",
    "    template_name = '__CMIMID__NODE__'\n",
    "    node[0] = template_name\n",
    "    new_tree = utils.deep_copy(tree)\n",
    "    node[0] = node_name\n",
    "    new_node = get_ref(new_tree, template_name)\n",
    "    new_node[0] = node_name\n",
    "    if node_name not in NODE_REGISTER:\n",
    "        NODE_REGISTER[node_name] = []\n",
    "    new_elt = (new_node, new_tree, executable, input_file,\n",
    "            {'inputstr': tree_to_str(new_tree), 'node':node, 'tree':tree})\n",
    "    NODE_REGISTER[node_name].append(new_elt)\n",
    "    return new_elt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect_nodes()` recursively calls `register_node()` on the tree so that all nodes are registered. The wrinkle here is that in some case such as parser combinators and peg parsers, there may be long chains of single child repetitions. i.e: `parse -> curry -> parse -> curry -> ...` etc. For them, if we have seen the first `parse` and `curry`, we do not gain anything by analyzing the remainign in the *same chain*. So, we mark such chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_nodes_single(node, tree, executable, inputfile, seen):\n",
    "    node_name, children, si, ei = node\n",
    "    elt = None\n",
    "    if is_node_method(node):\n",
    "        elt = register_node(node, tree, executable, inputfile)\n",
    "        if node_name in seen:\n",
    "            elt[4]['seen'] = seen[node_name]\n",
    "        else:\n",
    "            seen[node_name] = elt\n",
    "    if len(children) == 1:\n",
    "        collect_nodes_single(children[0], tree, executable, inputfile, seen)\n",
    "    else:\n",
    "        # no longer the single inheritance line.\n",
    "        for child in children:\n",
    "            collect_nodes(child, tree, executable, inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_nodes(node, tree, executable, inputfile):\n",
    "    node_name, children, si, ei = node\n",
    "    elt = None\n",
    "    if is_node_method(node):\n",
    "        elt = register_node(node, tree, executable, inputfile)\n",
    "    if len(children) == 1:\n",
    "        collect_nodes_single(children[0], tree, executable, inputfile, {node_name: elt})\n",
    "    else:\n",
    "        for child in children:\n",
    "            collect_nodes(child, tree, executable, inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking to see which methods are swappable, the idea is to choose a small sample set for a given node name, and check the current node against that sample set (swap both ways, and check the validity). The different validity patterns we get are marked as different kinds of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we identify the buckets based on one to one compatibility. Unfortunately, there is a problem here. Essentially, we assume that if `a` is compatible with `b`, and `b` is compatible with `c`, then `a` is compatible with `c`. However, this may not be true in all cases. See the limitations for instances when this assumption is invalidated. At this point, we have several options. The first is to do an $n \\times n$ comparison of all items in the bucket, in which case, we will have the true compatibility but with high computational cost. The next alternative is to choose a node in one bucket, and do the bucketing procedure again with the items in the particular bucket. This produces one more bit of information, and one can continue this prodcedure for larger and larger number of bits. One may also choose a statistical sample of $k$ items in the bucket, and go for a comparison only between $n \\times k$ items.\n",
    "\n",
    "At this point, we choose the fastest option, which gets us a reasonable accuracy. We use a single level classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_buckets(node_name):\n",
    "    all_elts = NODE_REGISTER[node_name]\n",
    "    # remove the duplicate nodes\n",
    "    elts = [e for e in all_elts if 'seen' not in e[4]]\n",
    "    seen_elts = [e for e in all_elts if 'seen' in e[4]]\n",
    "    first, *rest = elts\n",
    "    first[4]['pattern'] = 0\n",
    "    buckets = [first]\n",
    "    for enode in rest:\n",
    "        node0, tree0, executable0, inputfile0, _info0 = enode\n",
    "        a0 = node0, inputfile0, tree0\n",
    "        compatible = None\n",
    "        for bi, bnode in enumerate(buckets):\n",
    "            node1, tree1, executable1, inputfile1, _info1 = bnode\n",
    "            a1 = node1, inputfile1, tree1\n",
    "            result = is_compatible(a0, a1, executable0)\n",
    "            if result:\n",
    "                compatible = bi\n",
    "                enode[4]['pattern'] = bi\n",
    "                break\n",
    "        if compatible is None:\n",
    "            enode[4]['pattern'] = len(buckets)\n",
    "            buckets.append(enode)\n",
    "            \n",
    "    for e in seen_elts:\n",
    "        e_seen = e[4]['seen']\n",
    "        e_seen_pattern = e_seen[4]['pattern']\n",
    "        e[4]['pattern'] = e_seen_pattern\n",
    "    return {i:i for i,b in enumerate(buckets)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we identify that a node belongs to a particular pattern identifier, we update all the pseudo-methods belonging to that node. These can be found by simply traversiing the tree until the next method is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_method_stack(node, old_name, new_name):\n",
    "    nname, children, *rest = node\n",
    "    if not (':if_' in nname or ':while_' in nname):\n",
    "        return\n",
    "    method, ctrl, cname, num, can_empty, cstack = parse_pseudo_name(nname)\n",
    "    assert method == old_name, \"%s != %s\" % (method, old_name)\n",
    "    name = unparse_pseudo_name(new_name, ctrl, cname, num, can_empty, cstack)\n",
    "    #assert '?' not in name\n",
    "    node[0] = name\n",
    "    for c in node[1]:\n",
    "        update_method_stack(c, old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_method_name(k_m, my_id):\n",
    "    # fixup k_m with what is in my_id\n",
    "    original = k_m[0]\n",
    "    method, old_id = parse_method_name(original)\n",
    "    name = unparse_method_name(method, my_id)\n",
    "    k_m[0] = name\n",
    "\n",
    "    for c in k_m[1]:\n",
    "        update_method_stack(c, original[1:-1], name[1:-1])\n",
    "\n",
    "    return name, k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_original_method_names(node_name):\n",
    "    registered_xnodes = NODE_REGISTER[node_name]\n",
    "    for xnode in registered_xnodes:\n",
    "        # name it according to its pattern\n",
    "        nodeX, treeX, executableX, inputfileX, infoX = xnode\n",
    "        pattern = infoX['pattern']\n",
    "        update_method_name(infoX['node'], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to first collect and register all nodes by their names.\n",
    "Next, we sample N of these, and use the pattern of matches\n",
    "(**TODO**: Do we simply use the pattern of compatibility or the pattern\n",
    "of left to right replaceability -- that is, a is replaceable with b\n",
    "but b is not replaceable with a is 10 while full compatibility would\n",
    "be 11 -> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_method_trees(jtrees, log=False):\n",
    "    my_trees = []\n",
    "    for i,j in enumerate(jtrees):\n",
    "        tree = to_modifiable(j['tree']) # The tree ds.\n",
    "        executable = j['original']\n",
    "        inputfile = j['arg']\n",
    "        # we skip START\n",
    "        node_name, children, *rest = tree\n",
    "        assert node_name == '<START>'\n",
    "        for child in children:\n",
    "            collect_nodes(tree, tree, executable, inputfile)\n",
    "        my_trees.append({'tree':tree, 'original': executable, 'arg': inputfile})\n",
    "\n",
    "    for k in NODE_REGISTER:\n",
    "        identify_buckets(k)\n",
    "\n",
    "    # finally, update the original names.\n",
    "    for k in NODE_REGISTER:\n",
    "        if k == '<START>': continue\n",
    "        update_original_method_names(k)\n",
    "    return my_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_generalizer()\n",
    "mg_calc_trees = generalize_method_trees(mined_calc_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(v:=utils.display_tree(mg_calc_trees[0]['tree']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define how to update a pseudoname to a new id (when we detect a new pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pseudo_name(k_m, my_id):\n",
    "    # fixup k_m with what is in my_id\n",
    "    original = k_m[0]\n",
    "    method, ctrl, cid, altid, can_empty, method_stack = parse_pseudo_name(original)\n",
    "    if ctrl == 'if':\n",
    "        name = unparse_pseudo_name(method, ctrl, cid, \"%s.%d\" % (altid, my_id), can_empty, method_stack)\n",
    "    elif ctrl == 'while':\n",
    "        assert altid == '0'\n",
    "        name = unparse_pseudo_name(method, ctrl, cid, my_id, can_empty, method_stack)\n",
    "    else:\n",
    "        assert False\n",
    "    k_m[0] = name\n",
    "    return name, k_m\n",
    "\n",
    "def update_original_pseudo_names(node_name):\n",
    "    registered_xnodes = NODE_REGISTER[node_name]\n",
    "    for xnode in registered_xnodes:\n",
    "        # name it according to its pattern\n",
    "        nodeX, treeX, executableX, inputfileX, infoX = xnode\n",
    "        pattern = infoX['pattern']\n",
    "        update_pseudo_name(infoX['node'], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generalizing pseudonodes, we need to collect them just like we did for methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_pseudo_nodes(node, tree, executable, inputfile):\n",
    "    node_name, children, si, ei = node\n",
    "    if is_node_pseudo(node):\n",
    "        register_node(node, tree, executable, inputfile)\n",
    "\n",
    "    for child in children:\n",
    "        collect_pseudo_nodes(child, tree, executable, inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loops, we have a special processing that checks whether it can be deleted. If so, we would place `*` after their name. Else it is `+`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_the_loop_be_deleted(pattern, k, executable):\n",
    "    xnodes = [xnode for xnode in NODE_REGISTER[k] if xnode[-1]['pattern'] == pattern]\n",
    "    can_be_deleted = True\n",
    "    for xnode in xnodes:\n",
    "        node0, tree0, executable0, inputfile0, _info = xnode\n",
    "        a = is_a_replaceable_with_b((node0, '', tree0), (['', [], 0, 0], '', tree0), executable)\n",
    "        if not a:\n",
    "            can_be_deleted = False\n",
    "            break\n",
    "    for xnode in xnodes:\n",
    "        node0, tree0, executable0, inputfile0, info = xnode\n",
    "        method1, ctrl1, cname1, num1, can_empty, cstack1 = parse_pseudo_name(node0[0])\n",
    "        name = unparse_pseudo_name(method1, ctrl1, cname1, num1, Epsilon if can_be_deleted else NoEpsilon, cstack1)\n",
    "        info['node'][0] = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main `generalize_loop_trees()` generalizes loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_loop_trees(jtrees, log=False):\n",
    "    my_trees = []\n",
    "    for j in jtrees:\n",
    "        tree = to_modifiable(j['tree']) # The tree ds.\n",
    "        executable = j['original']\n",
    "        inputfile = j['arg']\n",
    "        # we skip START\n",
    "        node_name, children, *rest = tree\n",
    "        assert node_name == '<START>'\n",
    "        for child in children:\n",
    "            collect_pseudo_nodes(tree, tree, executable, inputfile)\n",
    "        my_trees.append({'tree':tree, 'original': executable, 'arg': inputfile})\n",
    "\n",
    "    for k in NODE_REGISTER:\n",
    "        patterns = identify_buckets(k)\n",
    "        for p in patterns:\n",
    "            can_the_loop_be_deleted(patterns[p], k, executable)\n",
    "\n",
    "    # finally, update the original names.\n",
    "    for k in NODE_REGISTER:\n",
    "        if k == '<START>': continue\n",
    "        update_original_pseudo_names(k)\n",
    "    return my_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_generalizer()\n",
    "lg_calc_trees = generalize_loop_trees(mg_calc_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(v:=utils.display_tree(lg_calc_trees[0]['tree']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayTree3(DisplayTree):\n",
    "    def extract_node(self, node, id):                                                      \n",
    "        symbol, children, *annotation = node                                         \n",
    "        return symbol, children, ''.join(str(a) for a in annotation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_generalizer()\n",
    "generalized_calc_trees = generalize_loop_trees(mg_calc_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(DisplayTree3(generalized_calc_trees[0]['tree']).display())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Grammar\n",
    "\n",
    "Generating a grammar from the generalized derivation trees is pretty simple. Start at the start node, and any node that represents a method or a pseudo method becomes a nonterminal. The children forms alternate expansions for the nonterminal. Since all the keys are compatible, merging the grammar is simply merging the hash map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a pretty printer for grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "RE_NONTERMINAL = re.compile(r'(<[^<> ]*>)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse_grammar(grammar, key, order, canonical):\n",
    "    rules = sorted(grammar[key])\n",
    "    old_len = len(order)\n",
    "    for rule in rules:\n",
    "        if not canonical:\n",
    "            res =  re.findall(RE_NONTERMINAL, rule)\n",
    "        else:\n",
    "            res = rule\n",
    "        for token in res:\n",
    "            if token.startswith('<') and token.endswith('>'):\n",
    "                if token not in order:\n",
    "                    order.append(token)\n",
    "    new = order[old_len:]\n",
    "    for ckey in new:\n",
    "        recurse_grammar(grammar, ckey, order, canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grammar(grammar, start_symbol='<START>', canonical=True):\n",
    "    order = [start_symbol]\n",
    "    recurse_grammar(grammar, start_symbol, order, canonical)\n",
    "    if len(order) != len(grammar.keys()):\n",
    "        assert len(order) < len(grammar.keys())\n",
    "    return {k: sorted(grammar[k]) for k in order}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees to grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grammar(tree, grammar):\n",
    "    node, children, _, _ = tree\n",
    "    if not children: return grammar\n",
    "    tokens = []\n",
    "    if node not in grammar:\n",
    "        grammar[node] = list()\n",
    "    for c in children:\n",
    "        tokens.append(c[0])\n",
    "        to_grammar(c, grammar)\n",
    "    grammar[node].append(tuple(tokens))\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_grammar(g1, g2):\n",
    "    all_keys = set(list(g1.keys()) + list(g2.keys()))\n",
    "    merged = {}\n",
    "    for k in all_keys:\n",
    "        alts = set(g1.get(k, []) + g2.get(k, []))\n",
    "        merged[k] = alts\n",
    "    return {k:[l for l in merged[k]] for k in merged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grammar(my_trees):\n",
    "    grammar = {}\n",
    "    ret = []\n",
    "    for my_tree in my_trees:\n",
    "        tree = my_tree['tree']\n",
    "        start = tree[0]\n",
    "        src_file = my_tree['original']\n",
    "        arg_file = my_tree['arg']\n",
    "        ret.append((start, src_file, arg_file))\n",
    "        g = to_grammar(tree, grammar)\n",
    "        grammar = merge_grammar(grammar, g)\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_grammar = convert_to_grammar(generalized_calc_trees)\n",
    "show_grammar(calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_1_GrammarFuzzer as limitfuzzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = limitfuzzer.LimitFuzzer(calc_grammar)\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Empty Alternatives for IF and Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to insert empty rules for those loops and conditionals that can be skipped. For loops, the entire sequence has to contain the empty marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_empty_rules(grammar):\n",
    "    new_grammar = {}\n",
    "    for k in grammar:\n",
    "        if k in ':if_':\n",
    "            name, marker = k.split('#')\n",
    "            if name.endswith(' *'):\n",
    "                new_grammar[k] = grammar[k].add(('',))\n",
    "            else:\n",
    "                new_grammar[k] = grammar[k]\n",
    "        elif k in ':while_':\n",
    "            # TODO -- we have to check the rules for sequences of whiles.\n",
    "            # for now, ignore.\n",
    "            new_grammar[k] = grammar[k]\n",
    "        else:\n",
    "            new_grammar[k] = grammar[k]\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_calc_grammar = check_empty_rules(calc_grammar)\n",
    "show_grammar(ne_calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = limitfuzzer.LimitFuzzer(ne_calc_grammar)\n",
    "for i in range(10):\n",
    "    print(repr(gf.fuzz(key='<START>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to generalize the loops. The idea is to look for patterns exclusively in the similarly named while loops using any of the regular expression learners. For the prototype, we replaced the modified Sequitur with the modified Fernau which gave us better regular expressions than before. The main constraint we have is that we want to avoid repeated execution of program if possible. Fernau algorithm can recover a reasonably approximate regular exression based only on positive data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The modified Fernau algorithm\n",
    "\n",
    "The Fernau algorithm is from _Algorithms for learning regular expressions from positive data_ by _HenningFernau_. Our algorithm uses a modified form of the Prefix-Tree-Acceptor from Fernau. First we define an LRF buffer of a given size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buf:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.items = [None] * self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add1()` takes in an array, and transfers the first element of the array into the end of current buffer, and simultaneously drops the first element of the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buf(Buf):\n",
    "    def add1(self, items):\n",
    "        self.items.append(items.pop(0))\n",
    "        return self.items.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For equality between the buffer and an array, we only compare when both the array and the items are actually elements and not chunked arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buf(Buf):\n",
    "    def __eq__(self, items):\n",
    "        if any(isinstance(i, dict) for i in self.items): return False\n",
    "        if any(isinstance(i, dict) for i in items): return False\n",
    "        return items == self.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detect_chunks()` detects any repeating portions of a list of `n` size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_chunks(n, lst_):\n",
    "    lst = list(lst_)\n",
    "    chunks = set()\n",
    "    last = Buf(n)\n",
    "    # check if the next_n elements are repeated.\n",
    "    for _ in range(len(lst) - n):\n",
    "        lnext_n = lst[0:n]\n",
    "        if last == lnext_n:\n",
    "            # found a repetition.\n",
    "            chunks.add(tuple(last.items))\n",
    "        else:\n",
    "            pass\n",
    "        last.add1(lst)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have detected plausible repeating sequences, we gather all similar sequences into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(lst_,n , chunks):\n",
    "    lst = list(lst_)\n",
    "    chunked_lst = []\n",
    "    while len(lst) >= n:\n",
    "        lnext_n = lst[0:n]\n",
    "        if (not any(isinstance(i, dict) for i in lnext_n)) and tuple(lnext_n) in chunks:\n",
    "            chunked_lst.append({'_':lnext_n})\n",
    "            lst = lst[n:]\n",
    "        else:\n",
    "            chunked_lst.append(lst.pop(0))\n",
    "    chunked_lst.extend(lst)\n",
    "    return chunked_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `identify_chunks()` simply calls the `detect_chunks()` on all given lists, and then converts all chunks identified into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_chunks(my_lsts):\n",
    "    # initialize\n",
    "    all_chunks = {}\n",
    "    maximum = max(len(lst) for lst in my_lsts)\n",
    "    for i in range(1, maximum//2+1):\n",
    "        all_chunks[i] = set()\n",
    "\n",
    "    # First, identify chunks in each list.\n",
    "    for lst in my_lsts:\n",
    "        for i in range(1,maximum//2+1):\n",
    "            chunks = detect_chunks(i, lst)\n",
    "            all_chunks[i] |= chunks\n",
    "\n",
    "    # Then, chunkify\n",
    "    new_lsts = []\n",
    "    for lst in my_lsts:\n",
    "        for i in range(1,maximum//2+1):\n",
    "            chunks = all_chunks[i]\n",
    "            lst = chunkify(lst, i, chunks)\n",
    "        new_lsts.append(lst)\n",
    "    return new_lsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prefix tree acceptor\n",
    "\n",
    "The prefix tree acceptor is a way to represent positive data. The `Node` class holds a single node in the prefix tree acceptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # Each tree node gets its unique id.\n",
    "    _uid = 0\n",
    "    def __init__(self, item):\n",
    "        # self.repeats = False\n",
    "        self.count = 1 # how many repetitions.\n",
    "        self.counters = set()\n",
    "        self.last = False\n",
    "        self.children = []\n",
    "        self.item = item\n",
    "        self.uid = Node._uid\n",
    "        Node._uid += 1\n",
    "\n",
    "    def update_counters(self):\n",
    "        self.counters.add(self.count)\n",
    "        self.count = 0\n",
    "        for c in self.children:\n",
    "            c.update_counters()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(\"(%s, [%s])\", (self.item, ' '.join([str(i) for i in self.children])))\n",
    "\n",
    "    def to_json(self):\n",
    "        s = (\"(%s)\" % ' '.join(self.item['_'])) if isinstance(self.item, dict) else str(self.item)\n",
    "        return (s, tuple(self.counters), [i.to_json() for i in self.children])\n",
    "\n",
    "    def inc_count(self):\n",
    "        self.count += 1\n",
    "\n",
    "    def add_ref(self):\n",
    "        self.count = 1\n",
    "\n",
    "    def get_child(self, c):\n",
    "        for i in self.children:\n",
    "            if i.item == c: return i\n",
    "        return None\n",
    "\n",
    "    def add_child(self, c):\n",
    "        # first check if it is the current node. If it is, increment\n",
    "        # count, and return ourselves.\n",
    "        if c == self.item:\n",
    "            self.inc_count()\n",
    "            return self\n",
    "        else:\n",
    "            # check if it is one of the children. If it is a child, then\n",
    "            # preserve its original count.\n",
    "            nc = self.get_child(c)\n",
    "            if nc is None:\n",
    "                nc = Node(c)\n",
    "                self.children.append(nc)\n",
    "            else:\n",
    "                nc.add_ref()\n",
    "            return nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update_tree()` essentially transforms a list of nodes to a chain of nodes starting at `root` if the `root` is an empty tree. If the `root` already contains a tree, the `update_tree()` traverses the path represented by `lst_` and makes a new child branch where the path specified doesn't exist in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tree(lst_, root):\n",
    "    lst = list(lst_)\n",
    "    branch = root\n",
    "    while lst:\n",
    "        first, *lst = lst\n",
    "        branch = branch.add_child(first)\n",
    "    branch.last = True\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a number of lists, the `create_tree_with_lists()` creates an actual tree out of these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree_with_lsts(lsts):\n",
    "    Node._uid = 0\n",
    "    root =  Node(None)\n",
    "    for lst in lsts:\n",
    "        root.count = 1 # there is at least one element.\n",
    "        update_tree(lst, root)\n",
    "        root.update_counters()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a node, and a key, return the key and alts as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_star(node, key):\n",
    "    if node.item is None:\n",
    "        return [], {}\n",
    "    if isinstance(node.item, dict):\n",
    "        # take care of counters\n",
    "        elements = node.item['_']\n",
    "        my_key = \"<%s-%d-s>\" % (key, node.uid)\n",
    "        alts = [elements]\n",
    "        if len(node.counters) > 1: # repetition\n",
    "            alts.append(elements + [my_key])\n",
    "        return [my_key], {my_key:alts}\n",
    "    else:\n",
    "        return [str(node.item)], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_to_grammar(node, grammar, key):\n",
    "    rule = []\n",
    "    alts = [rule]\n",
    "    if node.uid == 0:\n",
    "        my_key = \"<%s>\" % key\n",
    "    else:\n",
    "        my_key = \"<%s-%d>\" % (key, node.uid)\n",
    "    grammar[my_key] = alts\n",
    "    if node.item is not None:\n",
    "        mk, g = get_star(node, key)\n",
    "        rule.extend(mk)\n",
    "        grammar.update(g)\n",
    "    # is the node last?\n",
    "    if node.last:\n",
    "        assert node.item is not None\n",
    "        # add a duplicate rule that ends here.\n",
    "        ending_rule = list(rule)\n",
    "        # if there are no children, the current rule is\n",
    "        # any way ending.\n",
    "        if node.children:\n",
    "            alts.append(ending_rule)\n",
    "\n",
    "    if node.children:\n",
    "        if len(node.children) > 1:\n",
    "            my_ckey = \"<%s-%d-c>\" % (key, node.uid)\n",
    "            rule.append(my_ckey)\n",
    "            grammar[my_ckey] = [ [\"<%s-%d>\" % (key, c.uid)] for c in node.children]\n",
    "        else:\n",
    "            my_ckey = \"<%s-%d>\" % (key, node.children[0].uid)\n",
    "            rule.append(my_ckey)\n",
    "    else:\n",
    "        pass\n",
    "    for c in node.children:\n",
    "        node_to_grammar(c, grammar, key)\n",
    "    return grammar\n",
    "\n",
    "def generate_grammar(lists, key):\n",
    "    lsts = identify_chunks(lists)\n",
    "    tree = create_tree_with_lsts(lsts)\n",
    "    grammar = {}\n",
    "    node_to_grammar(tree, grammar, key)\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a rule, determine the abstraction for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_alts(rules, k):\n",
    "    ss = [[str(r) for r in rule] for rule in rules]\n",
    "    x = generate_grammar(ss, k[1:-1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_rules(grammar):\n",
    "    r_grammar = {}\n",
    "    for k in grammar:\n",
    "        new_grammar = collapse_alts(grammar[k], k)\n",
    "        # merge the new_grammar with r_grammar\n",
    "        # we know none of the keys exist in r_grammar because\n",
    "        # new keys are k prefixed.\n",
    "        for k_ in new_grammar:\n",
    "            r_grammar[k_] = new_grammar[k_]\n",
    "    return r_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_calc_grammar = collapse_rules(ne_calc_grammar)\n",
    "show_grammar(collapsed_calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = limitfuzzer.LimitFuzzer(collapsed_calc_grammar)\n",
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spaces_in_keys(grammar):\n",
    "    keys = {key: key.replace(' ', '_') for key in grammar}\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        new_alt = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = []\n",
    "            for t in rule:\n",
    "                for k in keys:\n",
    "                    t = t.replace(k, keys[k])\n",
    "                new_rule.append(t)\n",
    "            new_alt.append(new_rule)\n",
    "        new_grammar[keys[key]] = new_alt\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_grammar = convert_spaces_in_keys(collapsed_calc_grammar)\n",
    "show_grammar(calc_grammar, canonical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = limitfuzzer.LimitFuzzer(calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate and redundant entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_in_chain(token, chain):\n",
    "    while True:\n",
    "        if token in chain:\n",
    "            token = chain[token]\n",
    "            assert isinstance(token, str)\n",
    "        else:\n",
    "            break\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a new symbol for `grammar` based on `symbol_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_symbol(grammar, symbol_name=\"<symbol>\"):\n",
    "    if symbol_name not in grammar:\n",
    "        return symbol_name\n",
    "\n",
    "    count = 1\n",
    "    while True:\n",
    "        tentative_symbol_name = symbol_name[:-1] + \"-\" + repr(count) + \">\"\n",
    "        if tentative_symbol_name not in grammar:\n",
    "            return tentative_symbol_name\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace keys that have a single token definition with the token in the defition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement_candidate_chains(grammar, ignores):\n",
    "    to_replace = {}\n",
    "    for k in grammar:\n",
    "        if k in ignores: continue\n",
    "        if len(grammar[k]) != 1: continue\n",
    "        rule = grammar[k][0]\n",
    "        if len(rule) != 1: continue\n",
    "        if utils.is_nt(rule[0]):\n",
    "            to_replace[k] = rule[0]\n",
    "        else:\n",
    "            pass\n",
    "    return to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_by_new_key(grammar, keys_to_replace):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = [keys_to_replace.get(token, token)\n",
    "                        for token in rule]\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[keys_to_replace.get(key, key)] = new_rules\n",
    "    assert len(grammar) == len(new_grammar)\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_by_key(grammar, keys_to_replace):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        if key in keys_to_replace: continue\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            for t in rule:\n",
    "                assert isinstance(t, str)\n",
    "            new_rule = [first_in_chain(token, keys_to_replace) for token in rule]\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[key] = new_rules\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_entries(grammar):\n",
    "    keys_to_replace = replacement_candidate_chains(grammar, {start_symbol, '<main>'})\n",
    "    return replace_key_by_key(grammar, keys_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove keys that have similar rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_duplicate_rule_keys(grammar):\n",
    "    collect = {}\n",
    "    for k in grammar:\n",
    "        salt = str(sorted(grammar[k]))\n",
    "        if salt not in collect:\n",
    "            collect[salt] = (k, set())\n",
    "        else:\n",
    "            collect[salt][1].add(k)\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_rule_keys(grammar):\n",
    "    g = grammar\n",
    "    while True:\n",
    "        collect = collect_duplicate_rule_keys(g)\n",
    "        keys_to_replace = {}\n",
    "        for salt in collect:\n",
    "            k, st = collect[salt]\n",
    "            for s in st:\n",
    "                keys_to_replace[s] = k\n",
    "        if not keys_to_replace:\n",
    "            break\n",
    "        g = replace_key_by_key(g, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the control flow vestiges from names, and simply name them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_replacement_keys(grammar):\n",
    "    g = utils.deep_copy(grammar)\n",
    "    to_replace = {}\n",
    "    for k in grammar:\n",
    "        if ':' in k:\n",
    "            first, rest = k.split(':')\n",
    "            sym = new_symbol(g, symbol_name=first + '>')\n",
    "            assert sym not in g\n",
    "            g[sym] = None\n",
    "            to_replace[k] = sym\n",
    "        else:\n",
    "            continue\n",
    "    return to_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove keys that are referred to only from a single rule, and which have a single alternative.\n",
    "Import. This can't work on canonical representation. First, given a key, we figure out its distance to `<START>`.\n",
    "\n",
    "This is different from `remove_single_entries()` in that, there we do not care if the key is being used multiple times. Here, we only replace keys that are referred to only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_to_start(item, parents, start_symbol, seen=None):\n",
    "    if seen is None: seen = set()\n",
    "    if item in seen: return math.inf\n",
    "    seen.add(item)\n",
    "    if item == start_symbol: return 0\n",
    "    else: return 1 + min(len_to_start(p, parents, start_symbol, seen)\n",
    "                         for p in parents[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_length_to_start(items, parent_map, start_symbol):\n",
    "    return sorted(items, key=lambda i: len_to_start(i, parent_map, start_symbol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a map of `child -> [parents]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents_of_tokens(grammar, key, seen=None, parents=None):\n",
    "    if parents is None: parents, seen = {}, set()\n",
    "    if key in seen: return parents\n",
    "    seen.add(key)\n",
    "    for res in grammar[key]:\n",
    "        for token in res:\n",
    "            if not utils.is_nt(token): continue\n",
    "            parents.setdefault(token, []).append(key)\n",
    "    for ckey in {i for i in  grammar if i not in seen}:\n",
    "        get_parents_of_tokens(grammar, ckey, seen, parents)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_references(keys_to_replace):\n",
    "    to_process = list(keys_to_replace.keys())\n",
    "    updated_dict = {}\n",
    "    references = {}\n",
    "    order = []\n",
    "    while to_process:\n",
    "        key, *to_process = to_process\n",
    "        rule = keys_to_replace[key]\n",
    "        new_rule = []\n",
    "        skip = False\n",
    "        for token in rule:\n",
    "            if token not in updated_dict:\n",
    "                if token in to_process:\n",
    "                    # so this token will get defined later. We simply postpone\n",
    "                    # the processing of this key until that key is defined.\n",
    "                    # TODO: check for cycles.\n",
    "                    to_process.append(key)\n",
    "                    references.setdefault(token, set()).add(key)\n",
    "                    skip = True\n",
    "                    break\n",
    "                else:\n",
    "                    new_rule.append(token)\n",
    "            else:\n",
    "                new_rule.extend(updated_dict[token])\n",
    "        if not skip:\n",
    "            order.append(key)\n",
    "            updated_dict[key] = new_rule\n",
    "    return updated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_keys_by_rule(grammar, keys_to_replace):\n",
    "    # we now need to verify that none of the keys are part of the sequences.\n",
    "    keys_to_replace = remove_references(keys_to_replace)\n",
    "\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        if key in keys_to_replace: continue\n",
    "\n",
    "        new_rules = []\n",
    "        for rule in grammar[key]:\n",
    "            new_rule = []\n",
    "            for token in rule:\n",
    "                if token in keys_to_replace:\n",
    "                    new_rule.extend(keys_to_replace[token])\n",
    "                else:\n",
    "                    new_rule.append(token)\n",
    "            new_rules.append(new_rule)\n",
    "        new_grammar[key] = new_rules\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_alts(grammar, start_symbol):\n",
    "    single_alts = {p for p in grammar if len(grammar[p]) == 1 and p != start_symbol}\n",
    "\n",
    "    child_parent_map = get_parents_of_tokens(grammar, start_symbol)\n",
    "    assert len(child_parent_map) < len(grammar)\n",
    "\n",
    "    single_refs = {p:child_parent_map[p] for p in single_alts if len(child_parent_map[p]) <= 1}\n",
    "\n",
    "    ordered = order_by_length_to_start(single_refs, child_parent_map, start_symbol)\n",
    "\n",
    "    for p in ordered:\n",
    "        assert len(grammar[p]) == 1\n",
    "        if not isinstance(grammar[p][0], str):\n",
    "            print(p, grammar[p][0])\n",
    "\n",
    "    keys_to_replace = {p:grammar[p][0] for p in ordered}\n",
    "    g =  replace_keys_by_rule(grammar, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove similar rules from under a single key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_rule(r): return len(r)\n",
    "def len_definition(d): return sum([len_rule(r) for r in d])\n",
    "def len_grammar(g): return sum([len_definition(g[k]) for k in g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_rules_in_a_key(g):\n",
    "    g_ = {}\n",
    "    for k in g:\n",
    "        s = {str(r):r for r in g[k]}\n",
    "        g_[k] = list(sorted(list(s.values())))\n",
    "    return g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_gc(grammar, start_symbol):\n",
    "    def strip_key(grammar, key, order):\n",
    "        rules = sorted(grammar[key])\n",
    "        old_len = len(order)\n",
    "        for rule in rules:\n",
    "            for token in rule:\n",
    "                if utils.is_nt(token):\n",
    "                    if token not in order:\n",
    "                        order.append(token)\n",
    "        new = order[old_len:]\n",
    "        for ckey in new:\n",
    "            strip_key(grammar, ckey, order)\n",
    "\n",
    "    order = [start_symbol]\n",
    "    strip_key(grammar, start_symbol, order)\n",
    "    assert len(order) == len(grammar.keys())\n",
    "    g = {k: sorted(grammar[k]) for k in order}\n",
    "    for k in g:\n",
    "        for r in g[k]:\n",
    "            for t in r:\n",
    "                assert isinstance(t, str)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_grammar(g, start_symbol):\n",
    "    g = grammar_gc(g, start_symbol)\n",
    "    g1 = check_empty_rules(g) # add optional rules\n",
    "    g1 = grammar_gc(g1, start_symbol)\n",
    " \n",
    "    g2 = collapse_rules(g1) # learn regex\n",
    "    g2 = grammar_gc(g2, start_symbol)\n",
    "\n",
    "    g3 = convert_spaces_in_keys(g2) # fuzzable grammar\n",
    "    g3 = grammar_gc(g3, start_symbol)\n",
    "    return g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_entry_chains(grammar, start_symbol):\n",
    "    keys_to_replace = replacement_candidate_chains(grammar, {start_symbol, '<main>'})\n",
    "    return replace_key_by_key(grammar, keys_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_token_names(grammar):\n",
    "    keys_to_replace = collect_replacement_keys(grammar)\n",
    "    g = replace_key_by_new_key(grammar, keys_to_replace)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_definitions(g):\n",
    "    g_ = {}\n",
    "    for k in g:\n",
    "        rs_ = []\n",
    "        for r in g[k]:\n",
    "            assert not isinstance(r, str)\n",
    "            if len(r) == 1 and r[0] == k: continue\n",
    "            rs_.append(r)\n",
    "        g_[k] = rs_\n",
    "    return g_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_grammar(e, start_symbol):\n",
    "    assert start_symbol in e\n",
    "    l = len_grammar(e)\n",
    "    diff = 1\n",
    "    while diff > 0:\n",
    "        assert start_symbol in e\n",
    "        e = remove_single_entry_chains(e, start_symbol)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_duplicate_rule_keys(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = cleanup_token_names(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_single_alts(e, start_symbol)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_duplicate_rules_in_a_key(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        e = remove_self_definitions(e)\n",
    "        e = grammar_gc(e, start_symbol) # garbage collect\n",
    "\n",
    "        l_ = len_grammar(e)\n",
    "        diff = l - l_\n",
    "        l = l_\n",
    "    e = grammar_gc(e, start_symbol)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accio Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accio_grammar(fname, src, samples, cache=True):\n",
    "    hash_id = hashlib.md5(json.dumps(samples).encode()).hexdigest()\n",
    "    cache_file = \"build/%s_%s_generalized_tree.json\" % (fname, hash_id)\n",
    "    if os.path.exists(cache_file) and cache:\n",
    "        with open(cache_file) as f:\n",
    "            generalized_tree = json.load(f)\n",
    "    else:\n",
    "    # regenerate the program\n",
    "        program_src[fname] = src\n",
    "        with open('subjects/%s' % fname, 'w+') as f:\n",
    "            print(src, file=f) \n",
    "        resrc = rewrite(src, fname)\n",
    "        with open('build/%s' % fname, 'w+') as f:\n",
    "            print(resrc, file=f)\n",
    "        os.makedirs('samples/%s' % fname, exist_ok=True)\n",
    "        sample_files = {(\"samples/%s/%d.csv\"%(fname,i)):s for i,s in enumerate(samples)}\n",
    "        for k in sample_files:\n",
    "            with open(k, 'w+') as f:\n",
    "                print(sample_files[k], file=f)\n",
    "\n",
    "        call_trace = []\n",
    "        for i in sample_files:\n",
    "            thash_id = hashlib.md5(json.dumps(sample_files[i]).encode()).hexdigest()\n",
    "            trace_cache_file = \"build/%s_%s_trace.json\" % (fname, thash_id)\n",
    "            if os.path.exists(trace_cache_file) and cache:\n",
    "                with open(trace_cache_file) as f:\n",
    "                    my_tree = f.read()\n",
    "            else:\n",
    "                my_tree = utils.do(\"env PYTHONPATH='.:src:subjects' %s ./build/%s %s\" \n",
    "                                   % (sys.executable, fname, i), shell=True).stdout\n",
    "                with open(trace_cache_file, 'w+') as f:\n",
    "                    print(my_tree, file=f)\n",
    "            call_trace.append(json.loads(my_tree)[0])\n",
    "\n",
    "        mined_tree = miner(call_trace)\n",
    "        reset_generalizer()\n",
    "        generalized_m_tree = generalize_method_trees(mined_tree)\n",
    "        reset_generalizer()\n",
    "        generalized_tree = generalize_loop_trees(generalized_m_tree)\n",
    "        reset_generalizer()\n",
    "        # costly data structure.\n",
    "        with open(cache_file, 'w+') as f:\n",
    "            json.dump(generalized_tree, f)\n",
    "    g0 = convert_to_grammar(generalized_tree)\n",
    "    with open('build/%s_grammar_0.json' % fname, 'w+') as f:\n",
    "        json.dump(g0, f)\n",
    "    assert '<START>' in g0\n",
    "    g = cleanup_grammar(g0, start_symbol='<START>')\n",
    "    with open('build/%s_grammar_first.json' % fname, 'w+') as f:\n",
    "        json.dump(g, f)\n",
    "    g = compact_grammar(g, start_symbol='<START>')\n",
    "    with open('build/%s_grammar.json' % fname, 'w+') as f:\n",
    "        json.dump(g, f)\n",
    "    return show_grammar(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_grammar = accio_grammar('calculator.py', program_src['calculator.py'],\n",
    "                             ['(1+2)-2',\n",
    "                              '11',\n",
    "                              '3*5-22/3',\n",
    "                             '243+22*(112/990)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_grammar.keys())\n",
    "utils.display_grammar(calc_grammar, '<START>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = limitfuzzer.LimitFuzzer(calc_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(gf.fuzz(key='<START>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
