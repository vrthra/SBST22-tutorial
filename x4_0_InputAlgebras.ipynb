{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Algebras\n",
    "\n",
    "\n",
    "More complex and fully worked out examples including abstraction of real world bugs from Clojure, Rhino, Lua and a few Unix utilities [here](https://github.com/vrthra/FAlgebra/blob/master/src/FAlgebra.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_4_HierarchicalReducer as hdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_2_GrammarFuzzer as fuzzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_3_Parser as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.x0_4_HierarchicalReducer import PRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x3_0_AbstractingInputs as abstractinginputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our language is a simple language of boolean algebra. That is, it is the\n",
    "language of expressions in the specialization for a nonterminal such as `<A and(f1,f2)>`\n",
    "It is defined by the following grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.grammars as grammars\n",
    "utils.display_grammar(grammars.BEXPR_GRAMMAR, grammars.BEXPR_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a data structure to represent the boolean language.\n",
    "First we represent our literals using `LitB` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitB:\n",
    "    def __init__(self, a): self.a = a\n",
    "    def __str__(self): return self.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two boolean literals. The top and the bottom. The top literal\n",
    "also (T) essentially indicates that there is no specialization of the base\n",
    "nonterminal. For e.g. `<A>` is a top literal.\n",
    "Hence, we indicate it by an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueB = LitB('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottom literal indicates that there are no possible members for this\n",
    "particular nonterminal. For e.g. <A _|_> indicates that this is empty.\n",
    "We indicate it by the empty symbol _|_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FalseB = LitB('_|_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the standard terms of the boolean algebra. `or(.,.)`, `and(.,.)` and `neg(.)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrB:\n",
    "    def __init__(self, a): self.l = a\n",
    "    def __str__(self): return 'or(%s)' % ','.join(sorted([str(s) for s in self.l]))\n",
    "class AndB:\n",
    "    def __init__(self, a): self.l = a\n",
    "    def __str__(self): return 'and(%s)' % ','.join(sorted([str(s) for s in self.l]))\n",
    "class NegB:\n",
    "    def __init__(self, a): self.a = a\n",
    "    def __str__(self): return 'neg(%s)' % str(self.a)\n",
    "class BLit:\n",
    "    def __init__(self, a): self.a = a\n",
    "    def __str__(self): return str(self.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then come to the actual representative class. The class is initialized by\n",
    "providing it with a boolean expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BExpr:\n",
    "    def __init__(self, s):\n",
    "        if s is not None:\n",
    "            self._s = s\n",
    "            self._tree = self._parse(s)\n",
    "            self._simple, self._sympy = self._simplify()\n",
    "        else: # create\n",
    "            self._s = None\n",
    "            self._tree = None\n",
    "            self._simple = None\n",
    "            self._sympy = None\n",
    "\n",
    "    def _parse(self, k):\n",
    "        bexpr_parser = parser.EarleyParser(grammars.BEXPR_GRAMMAR)\n",
    "        bparse_tree = list(bexpr_parser.parse_on(k, start_symbol=grammars.BEXPR_START))[0]\n",
    "        bexpr = bparse_tree[1][0]\n",
    "        return bexpr\n",
    "\n",
    "    def _simplify(self):\n",
    "        return None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BExpr('and(and(f1,f2),f1)')\n",
    "utils.display_tree(b._tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define how to simplify boolean expressions. For example,\n",
    "we want to simplify `and(and(f1,f2),f1)` to just `and(f1,f2)`. Since this\n",
    "is already offered by `sympy` we use that.\n",
    "\n",
    "First we define a procedure that given the parse tree, converts it to a sympy\n",
    "expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BExpr(BExpr):\n",
    "    def _convert_to_sympy(self, bexpr_tree, symargs=None):\n",
    "        def get_op(node):\n",
    "            assert node[0] == '<bop>', node[0]\n",
    "            return ''.join([i[0] for i in node[1]])\n",
    "        if symargs is None:\n",
    "            symargs = {}\n",
    "        name, children = bexpr_tree\n",
    "        assert name == '<bexpr>', name\n",
    "        if len(children) == 1: # fault node\n",
    "            name = utils.tree_to_str(children[0])\n",
    "            if not name: return None, symargs\n",
    "            if name not in symargs:\n",
    "                symargs[name] = sympy.symbols(name)\n",
    "            return symargs[name], symargs\n",
    "        else:\n",
    "            operator = get_op(children[0])\n",
    "            if operator == 'and':\n",
    "                if children[2][0] == '<bexprs>':\n",
    "                    res = self._flatten(children[2])\n",
    "                else:\n",
    "                    res = [children[2]]\n",
    "                sp = [self._convert_to_sympy(a, symargs) for a in res]\n",
    "                return sympy.And(*[a for a,_ in sp]), symargs\n",
    "\n",
    "            elif operator == 'or':\n",
    "                if children[2][0] == '<bexprs>':\n",
    "                    res = self._flatten(children[2])\n",
    "                else:\n",
    "                    res = [children[2]]\n",
    "                sp = [self._convert_to_sympy(a, symargs) for a in res]\n",
    "                return sympy.Or(*[a for a,_ in sp]), symargs\n",
    "\n",
    "            elif operator == 'neg':\n",
    "                if children[2][0] == '<bexprs>':\n",
    "                    res = self._flatten(children[2])\n",
    "                else:\n",
    "                    res = [children[2]]\n",
    "                assert len(res) == 1\n",
    "                a,_ = self._convert_to_sympy(res[0], symargs)\n",
    "                return sympy.Not(a), symargs\n",
    "            else:\n",
    "                assert False\n",
    "\n",
    "    def _flatten(self, bexprs):\n",
    "        assert bexprs[0] == '<bexprs>'\n",
    "        if len(bexprs[1]) == 1:\n",
    "            return [bexprs[1][0]]\n",
    "        else:\n",
    "            assert len(bexprs[1]) == 3\n",
    "            a = bexprs[1][0]\n",
    "            comma = bexprs[1][1]\n",
    "            rest = bexprs[1][2]\n",
    "            return [a] + self._flatten(rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the reverse. Given the `sympy` expression, we define a\n",
    "procedure to convert it to the boolean data-structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BExpr(BExpr):\n",
    "    def _convert_sympy_to_bexpr(self, sexpr, log=False):\n",
    "        if isinstance(sexpr, sympy.Symbol):\n",
    "            return BLit(str(sexpr))\n",
    "        elif isinstance(sexpr, sympy.Not):\n",
    "            return NegB(self._convert_sympy_to_bexpr(sexpr.args[0]))\n",
    "        elif isinstance(sexpr, sympy.And):\n",
    "            a = sexpr.args[0]\n",
    "            b = sexpr.args[1]\n",
    "            if isinstance(a, sympy.Not):\n",
    "                if str(a.args[0]) == str(b): return FalseB # F & ~F == _|_\n",
    "            elif isinstance(b, sympy.Not):\n",
    "                if str(b.args[0]) == str(a): return FalseB # F & ~F == _|_\n",
    "            sym_vars = sorted([self._convert_sympy_to_bexpr(a) for a in sexpr.args], key=str)\n",
    "            assert sym_vars\n",
    "            if FalseB in sym_vars: return FalseB # if bottom is present in and, that is the result\n",
    "            if TrueB in sym_vars:\n",
    "                sym_vars = [s for s in sym_vars if s != TrueB] # base def does not do anything in and.\n",
    "                if not sym_vars: return TrueB\n",
    "            return AndB(sym_vars)\n",
    "        elif isinstance(sexpr, sympy.Or):\n",
    "            a = sexpr.args[0]\n",
    "            b = sexpr.args[1]\n",
    "            if isinstance(a, sympy.Not):\n",
    "                if str(a.args[0]) == str(b): return TrueB # F | ~F = U self._convert_sympy_to_bexpr(b)\n",
    "            elif isinstance(b, sympy.Not):\n",
    "                if str(b.args[0]) == str(a): return TrueB # F | ~F = U self._convert_sympy_to_bexpr(a)\n",
    "\n",
    "            sym_vars = sorted([self._convert_sympy_to_bexpr(a) for a in sexpr.args], key=str)\n",
    "            assert sym_vars\n",
    "            if TrueB in sym_vars: return TrueB # if original def is present in or, that is the result\n",
    "            if FalseB in sym_vars:\n",
    "                sym_vars = [s for s in sym_vars if s != FalseB]\n",
    "                if not sym_vars: return FalseB\n",
    "            return OrB(sym_vars)\n",
    "        else:\n",
    "            if log: print(repr(sexpr))\n",
    "            assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stitch them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BExpr(BExpr):\n",
    "    def simple(self):\n",
    "        if self._simple is None:\n",
    "            self._simple = str(self._convert_sympy_to_bexpr(self._sympy))\n",
    "        return self._simple\n",
    "\n",
    "    def _simplify(self):\n",
    "        e0, defs = self._convert_to_sympy(self._tree)\n",
    "        e1 = sympy.to_dnf(e0)\n",
    "        e2 = self._convert_sympy_to_bexpr(e1)\n",
    "        v = str(e2)\n",
    "        my_keys = [k for k in defs]\n",
    "        for k in my_keys:\n",
    "            del defs[k]\n",
    "        return v, e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BExpr('and(and(f1,f2),f1)')\n",
    "print(b.simple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to come to one of the main reasons for the existence of\n",
    "this class. In later posts, we will see that we will need to\n",
    "recreate a given nonterminal given the basic building blocks, and\n",
    "the boolean expression of the nonterminal. So, what we will do is\n",
    "to first parse the boolean expression using `BExpr`, then use\n",
    "`sympy` to simplify (as we have shown above), then unwrap the\n",
    "`sympy` one layer at a time, noting the operator used. When we\n",
    "come to the faults (or their negations) themselves, we return\n",
    "back from negation with their definitions from the original grammars,\n",
    "and as we return from each layer, we reconstruct the required\n",
    "expression from the given nonterminal definitions (or newly built ones)>\n",
    "\n",
    "The `get_operator()` returns the\n",
    "outer operator, `op_fst()` returns the first operator if the\n",
    "operator was a negation (and throws exception if it is used\n",
    "otherwise, and `op_fst_snd()` returns the first and second\n",
    "parameters for the outer `and` or `or`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BExpr(BExpr):\n",
    "    def get_operator(self):\n",
    "        if isinstance(self._sympy, sympy.And): return 'and'\n",
    "        elif isinstance(self._sympy, sympy.Or): return 'or'\n",
    "        elif isinstance(self._sympy, sympy.Not): return 'neg'\n",
    "        else: return ''\n",
    "\n",
    "    def op_fst(self):\n",
    "        op = self.get_operator()\n",
    "        assert op == 'neg'\n",
    "        bexpr = BExpr(None)\n",
    "        bexpr._sympy = self._sympy.args[0]\n",
    "        return bexpr\n",
    "\n",
    "    def op_fst_snd(self):\n",
    "        bexpr = BExpr(None)\n",
    "        bexpr._sympy = self._sympy.args[0]\n",
    "\n",
    "        bexpr_rest = BExpr(None)\n",
    "        op = self.get_operator()\n",
    "\n",
    "        if op == 'and':\n",
    "            bexpr_rest._sympy = sympy.And(*self._sympy.args[1:])\n",
    "        elif op == 'or':\n",
    "            bexpr_rest._sympy = sympy.Or(*self._sympy.args[1:])\n",
    "        else:\n",
    "            assert False\n",
    "        return bexpr, bexpr_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define two convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BExpr(BExpr):\n",
    "    def with_key(self, k):\n",
    "        s = self.simple()\n",
    "        if s:\n",
    "            return '<%s %s>' % (stem(k), s)\n",
    "        else:\n",
    "            # this bexpr does not contain an expression.\n",
    "            # So return the basic key\n",
    "            return normalize(k)\n",
    "\n",
    "    def negate(self):\n",
    "        bexpr = BExpr(None)\n",
    "        bexpr._sympy = sympy.Not(self._sympy).simplify()\n",
    "        return bexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, given a grammar, we need to find all undefined nonterminals that\n",
    "we need to reconstruct. This is done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_nonterminals(g):\n",
    "    lst = []\n",
    "    for k in g:\n",
    "        for r in g[k]:\n",
    "            for t in r:\n",
    "                if utils.is_nt(t):\n",
    "                    lst.append(t)\n",
    "    return list(sorted(set(lst)))\n",
    "\n",
    "def undefined_keys(grammar):\n",
    "    keys = find_all_nonterminals(grammar)\n",
    "    return [k for k in keys if k not in grammar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_GRAMMAR = {\n",
    " '<start>': [['<expr>']],\n",
    " '<expr>': [['<term>', '+', '<expr>'],\n",
    "            ['<term>', '-', '<expr>'],\n",
    "            ['<term>']],\n",
    " '<term>': [['<factor>', '*', '<term>'],\n",
    "            ['<factor>', '/', '<term>'],\n",
    "            ['<factor>']],\n",
    " '<factor>': [['+', '<factor>'],\n",
    "              ['-', '<factor>'],\n",
    "              ['(', '<expr>', ')'],\n",
    "              ['<integer>', '.', '<integer>'],\n",
    "              ['<integer>']],\n",
    " '<integer>': [['<digit>', '<integer>'], ['<digit>']],\n",
    " '<digit>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']]}\n",
    "\n",
    "EXPR_START = '<start>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_DPAREN_S = '<start D1>'\n",
    "EXPR_DPAREN_G = {\n",
    "        '<start>': [['<expr>']],\n",
    "        '<expr>': [['<term>', '+', '<expr>'], ['<term>', '-', '<expr>'], ['<term>']],\n",
    "        '<term>': [['<factor>', '*', '<term>'], ['<factor>', '/', '<term>'], ['<factor>']],\n",
    "        '<factor>': [['+', '<factor>'], ['-', '<factor>'], ['(', '<expr>', ')'], ['<integer>', '.', '<integer>'], ['<integer>']],\n",
    "        '<integer>': [['<digit>', '<integer>'], ['<digit>']],\n",
    "        '<digit>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']],\n",
    "        '<factor D1_0>': [['(', '<expr D1_1>', ')']],\n",
    "        '<expr D1_1>': [['<term D1_2>']],\n",
    "        '<term D1_2>': [['<factor D1_3>']],\n",
    "        '<factor D1_3>': [['(', '<expr>', ')']],\n",
    "        '<start D1>': [['<expr D1>']],\n",
    "        '<expr D1>': [['<term D1>', '+', '<expr>'], ['<term>', '+', '<expr D1>'], ['<term D1>', '-', '<expr>'], ['<term>', '-', '<expr D1>'], ['<term D1>']],\n",
    "        '<term D1>': [['<factor D1>', '*', '<term>'], ['<factor>', '*', '<term D1>'], ['<factor D1>', '/', '<term>'], ['<factor>', '/', '<term D1>'], ['<factor D1>']],\n",
    "        '<factor D1>': [['+', '<factor D1>'], ['-', '<factor D1>'], ['(', '<expr D1>', ')'], ['(', '<expr D1_1>', ')']]\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractinginputs.summarize_fault_grammar(EXPR_DPAREN_G, EXPR_DPAREN_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_DZERO_S = '<start Z1>'\n",
    "EXPR_DZERO_G = {\n",
    "        '<start>': [['<expr>']],\n",
    "        '<expr>': [['<term>', '+', '<expr>'], ['<term>', '-', '<expr>'], ['<term>']],\n",
    "        '<term>': [['<factor>', '*', '<term>'], ['<factor>', '/', '<term>'], ['<factor>']],\n",
    "        '<factor>': [['+', '<factor>'], ['-', '<factor>'], ['(', '<expr>', ')'], ['<integer>', '.', '<integer>'], ['<integer>']],\n",
    "        '<integer>': [['<digit>', '<integer>'], ['<digit>']],\n",
    "        '<digit>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']],\n",
    "        '<term Z1_0>': [['<factor>', '/', '<term Z1_1>']],\n",
    "        '<term Z1_1>': [['<factor Z1_2>']],\n",
    "        '<factor Z1_2>': [['<integer Z1_3>']],\n",
    "        '<integer Z1_3>': [['<digit Z1_4>']],\n",
    "        '<digit Z1_4>': [['0']],\n",
    "        '<start Z1>': [['<expr Z1>']],\n",
    "        '<expr Z1>': [['<term Z1>', '+', '<expr>'], ['<term>', '+', '<expr Z1>'], ['<term Z1>', '-', '<expr>'], ['<term>', '-', '<expr Z1>'], ['<term Z1>']],\n",
    "        '<term Z1>': [['<factor Z1>', '*', '<term>'], ['<factor>', '*', '<term Z1>'], ['<factor Z1>', '/', '<term>'], ['<factor>', '/', '<term Z1>'], ['<factor Z1>'], ['<factor>', '/', '<term Z1_1>']],\n",
    "        '<factor Z1>': [['+', '<factor Z1>'], ['-', '<factor Z1>'], ['(', '<expr Z1>', ')']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractinginputs.summarize_fault_grammar(EXPR_DZERO_G, EXPR_DZERO_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar ={**EXPR_DZERO_G, **EXPR_DPAREN_G, **{'<start and(D1,Z1)>': [['<expr and(D1,Z1)>']]}}\n",
    "keys = undefined_keys(grammar)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplit(token):\n",
    "    assert token[0], token[-1] == ('<', '>')\n",
    "    front, *back = token[1:-1].split(None, 1)\n",
    "    return front, ' '.join(back)\n",
    "\n",
    "def refinement(token):\n",
    "    return tsplit(token)[1].strip()\n",
    "\n",
    "def is_refined_key(key):\n",
    "    assert utils.is_nt(key)\n",
    "    return (' ' in key)\n",
    "\n",
    "def is_base_key(key):\n",
    "    return not is_refined_key(key)\n",
    "\n",
    "def stem(token):\n",
    "    return tsplit(token)[0].strip()\n",
    "\n",
    "def normalize(token):\n",
    "    assert utils.is_nt(token), token\n",
    "    if is_base_key(token): return token\n",
    "    return '<%s>' % stem(token)\n",
    "\n",
    "def refine_base_key(k, prefix):\n",
    "    assert utils.is_nt(k), k\n",
    "    assert is_base_key(k), k\n",
    "    return '<%s %s>' % (stem(k), prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Grammars\n",
    "Now, we want to combine these grammars. Remember that a gramamr has a set of\n",
    "definitions that correspond to nonterminals, and each definition has a set of\n",
    "rules. We start from the rules. If we want to combine two grammars, we need\n",
    "to make sure that any input produced from the combined grammar is also parsed\n",
    "by the original grammars. That is, any rule from the combined grammar should\n",
    "have a corresponding rule in the original grammars. This gives us the\n",
    "algorithm for combining two rules. First, we can only combine rules that have\n",
    "similar base representation. That is, if ruleA is `[<A f1>, <B f2>, 'T']` \n",
    "where `<A>` and `<B>` are nonterminals and `T` is a terminal\n",
    "and ruleB is `[<A f1>, <C f3>]`, these can't have a combination in the\n",
    "combined grammar. On the other hand, if ruleB is `[<A f3>, <B f4> 'T']`\n",
    "then, a combined rule of `[<A f1 & f3>, <B f2 & f4>, 'T']` can infact\n",
    "represent both parent rules. That is, when combining two rules from different,\n",
    "grammars, their combination is empty if they have different base\n",
    "representation.\n",
    "\n",
    "The idea for combining two definitions of nonterminals is simply using the\n",
    "distributive law. A definition is simply # `A1 or B1 or C1` where `A1` etc are\n",
    "rules. Now, when you want to and two defintions, you have\n",
    "`and(A1 or B1 or C1, A2 or B2 or C2)` , and you want the `or` out again.\n",
    "So, this becomes\n",
    "\n",
    "```\n",
    "(A1 AND A2) OR (A1 AND B2) OR (A1 AND C2) OR\n",
    "(A2 AND B1) OR (A2 AND C1) OR\n",
    "(B1 AND B2) OR (B1 AND C2) OR\n",
    "(B2 AND C1) OR (C1 AND C2)\n",
    "```\n",
    "\n",
    "which is essentially that many rules.\n",
    "\n",
    "### Combining tokens\n",
    "If they have the same base representation, then we only have to deal with how\n",
    "to combine the nonterminal symbols. The terminal symbols are exactly the same\n",
    "in parent rules as well as combined rule. So, given two tokens, we can\n",
    "combine them as follows. The `and` of a refined nonterminal and a base\n",
    "nonterminal is always the refined nonterminal. Otherwise, it is simply an\n",
    "`and()` specialization of both refinements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_nonterminals(k1, k2):\n",
    "    b1, s1 = tsplit(k1)\n",
    "    b2, s2 = tsplit(k2)\n",
    "    assert b1 == b2\n",
    "    if not s1: return k2\n",
    "    if not s2: return k1\n",
    "    if s1 == s2: return k1\n",
    "    return '<%s and(%s,%s)>' % (b1, s1, s2)\n",
    "\n",
    "def and_tokens(t1, t2):\n",
    "    if not utils.is_nt(t1): return t1\n",
    "    return and_nonterminals(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(and_tokens('C', 'C'))\n",
    "print(and_tokens('<A>', '<A f1>'))\n",
    "print(and_tokens('<A f2>', '<A f1>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining rules\n",
    "Next, we define combination for rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_rules(ruleA, ruleB):\n",
    "    AandB_rule = []\n",
    "    for t1,t2 in zip(ruleA, ruleB):\n",
    "        AandB_rule.append(and_tokens(t1, t2))\n",
    "    return AandB_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(and_rules(['<A>', '<B f1>', 'C'], ['<A f1>', '<B>', 'C']))\n",
    "print(and_rules(['<A f2>', '<B f1>', 'C'], ['<A f1>', '<B f3>', 'C']))\n",
    "print(and_rules(['<A f1>', '<B f1>', 'C'], ['<A f1>', '<B f3>', 'C']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining rulesets\n",
    "\n",
    "Next, our grammars may contain multiple rules that represent the same base\n",
    "rule. All the rules that represent the same base rule is called a ruleset.\n",
    "combining two rulesets is done by producing a new ruleset that contains all\n",
    "possible pairs of rules from the parent ruleset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_ruleset(rulesetA, rulesetB):\n",
    "    rules = []\n",
    "    for ruleA,ruleB in I.product(rulesetA, rulesetB):\n",
    "        AandB_rule = and_rules(ruleA, ruleB)\n",
    "        rules.append(AandB_rule)\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [['<A>', '<B f1>', 'C'], ['<A f1>', '<B>', 'C']]\n",
    "B = [['<A f2>', '<B f1>', 'C'], ['<A f1>', '<B f3>', 'C']]\n",
    "C = [['<A f1>', '<B f1>', 'C'], ['<A f1>', '<B f3>', 'C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in and_ruleset(A, B): print(k)\n",
    "print()\n",
    "for k in and_ruleset(A, C): print(k)\n",
    "print()\n",
    "for k in and_ruleset(B, C): print(k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a few helper functions that collects all rulesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(key):\n",
    "    if is_base_key(key): return key\n",
    "    return '<%s>' % stem(key)\n",
    "\n",
    "def normalize_grammar(g):\n",
    "    return {normalize(k):list({tuple([normalize(t) if utils.is_nt(t) else t for t in r]) for r in g[k]}) for k in g}\n",
    "\n",
    "def rule_to_normalized_rule(rule):\n",
    "    return [normalize(t) if utils.is_nt(t) else t for t in rule]\n",
    "\n",
    "def normalized_rule_match(r1, r2):\n",
    "    return rule_to_normalized_rule(r1) == rule_to_normalized_rule(r2)\n",
    "\n",
    "def rule_normalized_difference(rulesA, rulesB):\n",
    "    rem_rulesA = rulesA\n",
    "    for ruleB in rulesB:\n",
    "        rem_rulesA = [rA for rA in rem_rulesA if not normalized_rule_match(rA, ruleB)]\n",
    "    return rem_rulesA\n",
    "\n",
    "def get_rulesets(rules):\n",
    "    rulesets = {}\n",
    "    for rule in rules:\n",
    "        nr = tuple(rule_to_normalized_rule(rule))\n",
    "        if nr not in rulesets: rulesets[nr] = []\n",
    "        rulesets[nr].append(rule)\n",
    "    return rulesets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition conjunction\n",
    "Now, we can define the conjunction of definitions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_definitions(rulesA, rulesB):\n",
    "    AandB_rules = []\n",
    "    rulesetsA, rulesetsB = get_rulesets(rulesA), get_rulesets(rulesB)\n",
    "    # drop any rules that are not there in both.\n",
    "    keys = set(rulesetsA.keys()) & set(rulesetsB.keys())\n",
    "    for k in keys:\n",
    "        new_rules = and_ruleset(rulesetsA[k], rulesetsB[k])\n",
    "        AandB_rules.extend(new_rules)\n",
    "    return AandB_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr1 = [r for k in EXPR_DPAREN_G if 'expr' in k for r in EXPR_DPAREN_G[k]]\n",
    "expr2 = [r for k in EXPR_DZERO_G if 'expr' in k for r in EXPR_DZERO_G[k]]\n",
    "for k in and_definitions(expr1, expr2):\n",
    "    print(k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grammar conjunction\n",
    "We can now define our grammar conjunction as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_grammars_(g1, s1, g2, s2):\n",
    "    g1_keys = g1.keys()\n",
    "    g2_keys = g2.keys()\n",
    "    g = {**g1, **g2}\n",
    "    for k1,k2 in I.product(g1_keys, g2_keys):\n",
    "        if normalize(k1) != normalize(k2): continue\n",
    "        and_key = and_tokens(k1, k2)\n",
    "        g[and_key] = and_definitions(g1[k1], g2[k2])\n",
    "    return g, and_tokens(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_g, combined_s = utils.grammar_gc(*and_grammars_(EXPR_DPAREN_G, EXPR_DPAREN_S, EXPR_DZERO_G, EXPR_DZERO_S))\n",
    "utils.display_grammar(combined_g, combined_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grammar is now guaranteed to produce instances of both characterizing\n",
    "subtrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expr_div_by_zero(input_str):\n",
    "    if '/0)' in input_str: return hdd.PRes.success\n",
    "    else: return hdd.PRes.failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expr_double_paren(inp):\n",
    "    if re.match(r'.*[(][(].*[)][)].*', inp):\n",
    "        return PRes.success\n",
    "    return PRes.failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_f = fuzzer.LimitFuzzer(combined_g)\n",
    "for i in range(10):\n",
    "    v = combined_f.iter_gen_key(key=combined_s, max_depth=10)\n",
    "    s = utils.tree_to_str(v)\n",
    "    assert expr_div_by_zero(s)\n",
    "    assert expr_double_paren(s)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing inputs with at least one of the two fault inducing fragments guaranteed to be present.\n",
    "How do we construct grammars that are guaranteed to contain at least one of\n",
    "the evocative patterns? This is actually much less complicated than `and`\n",
    "\n",
    "The idea is simply using the distributive law. A definition is simply\n",
    "`A1 or B1 or C1` as before where `A1` etc are rules.\n",
    "Now, when you want to `or` two definitions, you have\n",
    "`or(A1 or B1 or C1, A2 or B2 or C2)`, then it simply becomes\n",
    "`A1 or B1 or C1 or A2 or B2 or C2`\n",
    "At this point, our work is essentially done. All that we need to do\n",
    "is to merge any rules that potentially allow us to merge. However, this\n",
    "is not compulsory.\n",
    "\n",
    "### Nonterminals\n",
    "For nonterminals, it is similar to `and` except that the base cases differ.\n",
    "`or` of a base nonterminal with a refined nonterminal is always the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_nonterminals(k1, k2):\n",
    "    b1, s1 = tsplit(k1)\n",
    "    b2, s2 = tsplit(k2)\n",
    "    assert b1 == b2\n",
    "    if not s1: return k1\n",
    "    if not s2: return k2\n",
    "    if s1 == s2: return k1\n",
    "    return '<%s or(%s,%s)>' % (b1, s1, s2)\n",
    "\n",
    "def or_tokens(t1, t2):\n",
    "    if not utils.is_nt(t1): return t1\n",
    "    return or_nonterminals(t1, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rules\n",
    "What rules can be merged? Only those rules can be merged that has\n",
    "a single refinement difference. That is if we have\n",
    "`or(<A 1> <B 5> <C>, <A 2> <B 5> <C>)`, then this merges to\n",
    "`<A or(1,2)><B 5><C>`. However `or(<A 1> <B 5> <C>, <A 2> <B 6> <C>)`\n",
    "is not mergeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_rules(ruleA, ruleB, merge_with_or=True):\n",
    "    pos = []\n",
    "    for i,(t1,t2) in enumerate(zip(ruleA, ruleB)):\n",
    "        if t1 == t2: continue\n",
    "        else: pos.append(i)\n",
    "    if len(pos) == 0: return [ruleA]\n",
    "    elif len(pos) == 1:\n",
    "        result = [[or_tokens(ruleA[i], ruleB[i]) if i == pos[0] else t\n",
    "                for i,t in enumerate(ruleA)]]\n",
    "        if merge_with_or: return result\n",
    "        # if what we have at pos[0] is a base key, then we return the\n",
    "        # rule. else we do not.\n",
    "        merged = result[0][pos[0]]\n",
    "        if is_base_key(merged):\n",
    "            return result\n",
    "        else:\n",
    "            return [ruleA, ruleB]\n",
    "    else: return [ruleA, ruleB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = ['<A 1>', '<B>','<C>']\n",
    "a2 = ['<A 2>', '<B>','<C>']\n",
    "for r in or_rules(a1, a2): print(r)\n",
    "print()\n",
    "a3 = ['<A 1>', '<B 2>','<C>']\n",
    "a4 = ['<A 1>', '<B 3>','<C>']\n",
    "for r in or_rules(a3, a4): print(r)\n",
    "print()\n",
    "a5 = ['<A 1>', '<B 2>','<C 3>']\n",
    "a6 = ['<A 1>', '<B 3>','<C>']\n",
    "for r in or_rules(a5, a6): print(r)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rulesets\n",
    "For `or` rulesets we first combine\n",
    "both rulesets together then (optional) take one at a time,\n",
    "and check if it can be merged with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_ruleset(rulesetA, rulesetB, merge_with_or=True):\n",
    "    rule,*rules = (rulesetA + rulesetB)\n",
    "    current_rules = [rule]\n",
    "    while rules:\n",
    "        rule,*rules = rules\n",
    "        new_rules = []\n",
    "        modified = False\n",
    "        for i,r in enumerate(current_rules):\n",
    "            v =  or_rules(r, rule, merge_with_or)\n",
    "            if len(v) == 1:\n",
    "                current_rules[i] = v[0]\n",
    "                rule = None\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        if rule is not None:\n",
    "            current_rules.append(rule)\n",
    "    return current_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [['<A>', '<B f1>', 'C'], ['<A f1>', '<B>', 'C']]\n",
    "B = [['<A>', '<B f2>', 'C'], ['<A f1>', '<B f3>', 'C']]\n",
    "for k in or_ruleset(A, B): print(k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition disjunction\n",
    "Now, we can define the disjunction of definitions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_definitions(rulesA, rulesB, merge_with_or=True):\n",
    "    AorB_rules = []\n",
    "    rulesetsA, rulesetsB = get_rulesets(rulesA), get_rulesets(rulesB)\n",
    "    keys = set(rulesetsA.keys()) | set(rulesetsB.keys())\n",
    "    for k in keys:\n",
    "        new_rules = or_ruleset(rulesetsA.get(k, []), rulesetsB.get(k, []), merge_with_or)\n",
    "        AorB_rules.extend(new_rules)\n",
    "    return AorB_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr1 = [r for k in EXPR_DPAREN_G if 'expr' in k for r in EXPR_DPAREN_G[k]]\n",
    "expr2 = [r for k in EXPR_DZERO_G if 'expr' in k for r in EXPR_DZERO_G[k]]\n",
    "for k in or_definitions(expr1, expr2):\n",
    "    print(k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grammar disjunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_grammars_(g1, s1, g2, s2):\n",
    "    g = {}\n",
    "    # now get the matching keys for each pair.\n",
    "    for k in list(g1.keys()) + list(g2.keys()): \n",
    "         g[k] = [[t for t in r] for r in list(set([tuple(k) for k in (g1.get(k, []) + g2.get(k, []))]))]\n",
    "    \n",
    "    # We do not actually need to use merging of rule_sets for disjunction.\n",
    "    s_or = or_nonterminals(s1, s2)\n",
    "    g[s_or] = g1[s1] + g2[s2]\n",
    "    return g, s_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_g, or_s = utils.grammar_gc(*or_grammars_(EXPR_DPAREN_G, EXPR_DPAREN_S, EXPR_DZERO_G, EXPR_DZERO_S))\n",
    "utils.display_grammar(or_g, or_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "or_f = fuzzer.LimitFuzzer(or_g)\n",
    "for i in range(10):\n",
    "    v_ = or_f.iter_gen_key(key=or_s, max_depth=10)\n",
    "    v = utils.tree_to_str(v_)\n",
    "    assert (expr_div_by_zero(v) or expr_double_paren(v))\n",
    "    print(v)\n",
    "    if expr_div_by_zero(v) == hdd.PRes.success: print('>', 1)\n",
    "    if expr_double_paren(v) == hdd.PRes.success: print('>',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will see how to reconstruct grammars given the building blocks.\n",
    "Our `reconstruct_rules_from_bexpr()` is a recursive procedure that will take a\n",
    "given key, the corresponding refinement in terms of a `BExpr()` instance, the\n",
    "grammar containing the nonterminals, and it will attempt to reconstruct the\n",
    "key definition from the given nonterminals,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructRules:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    def reconstruct_rules_from_bexpr(self, key, bexpr):\n",
    "        f_key = bexpr.with_key(key)\n",
    "        if f_key in self.grammar:\n",
    "            return self.grammar[f_key], f_key\n",
    "        else:\n",
    "            operator = bexpr.get_operator()\n",
    "            if operator == 'and':\n",
    "                return self.reconstruct_and_bexpr(key, bexpr)\n",
    "            elif operator == 'or':\n",
    "                return self.reconstruct_or_bexpr(key, bexpr)\n",
    "            elif operator == 'neg':\n",
    "                return self.reconstruct_neg_bexpr(key, bexpr)\n",
    "            else:\n",
    "                return self.reconstruct_orig_bexpr(key, bexpr)\n",
    "\n",
    "    def reconstruct_orig_bexpr(self, key, bexpr):\n",
    "        assert False\n",
    "\n",
    "    def reconstruct_neg_bexpr(self, key, bexpr):\n",
    "        assert False\n",
    "\n",
    "    def reconstruct_and_bexpr(self, key, bexpr):\n",
    "        fst, snd = bexpr.op_fst_snd()\n",
    "        assert fst != snd\n",
    "        f_key = bexpr.with_key(key)\n",
    "        d1, s1 = self.reconstruct_rules_from_bexpr(key, fst)\n",
    "        d2, s2 = self.reconstruct_rules_from_bexpr(key, snd)\n",
    "        and_rules = and_definitions(d1, d2)\n",
    "        return and_rules, f_key\n",
    "\n",
    "    def reconstruct_or_bexpr(self, key, bexpr):\n",
    "        fst, snd = bexpr.op_fst_snd()\n",
    "        f_key = bexpr.with_key(key)\n",
    "        d1, s1 = self.reconstruct_rules_from_bexpr(key, fst)\n",
    "        assert fst != snd\n",
    "        d2, s2 = self.reconstruct_rules_from_bexpr(key, snd)\n",
    "        or_rules = or_definitions(d1, d2)\n",
    "        return or_rules, f_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bexpr = BExpr('and(D1,Z1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar ={**EXPR_DZERO_G, **EXPR_DPAREN_G, **EXPR_GRAMMAR}\n",
    "rr = ReconstructRules(grammar)\n",
    "d1, s1 = rr.reconstruct_rules_from_bexpr('<start>', my_bexpr)\n",
    "grammar[s1] = d1\n",
    "remaining = undefined_keys(grammar)\n",
    "print(d1,s1)\n",
    "print(\"remaining:\", remaining)\n",
    "rr = ReconstructRules({**grammar, **{s1:d1}})\n",
    "d2, s2 = rr.reconstruct_rules_from_bexpr(remaining[0], my_bexpr)\n",
    "grammar[s2] = d2\n",
    "remaining = undefined_keys(grammar)\n",
    "print(d2,s2)\n",
    "print(\"remaining:\", remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bexpr = BExpr('or(D1,Z1)')\n",
    "grammar ={**EXPR_DZERO_G, **EXPR_DPAREN_G, **EXPR_GRAMMAR}\n",
    "rr = ReconstructRules(grammar)\n",
    "d1, s1 = rr.reconstruct_rules_from_bexpr('<start>', my_bexpr)\n",
    "grammar[s1] = d1\n",
    "remaining = undefined_keys(grammar)\n",
    "print(d1,s1)\n",
    "print(\"remaining:\", remaining)\n",
    "rr = ReconstructRules({**grammar, **{s1:d1}})\n",
    "d2, s2  = rr.reconstruct_rules_from_bexpr(remaining[0], my_bexpr)\n",
    "grammar[s2] = d2\n",
    "remaining = undefined_keys(grammar)\n",
    "print(d2,s2)\n",
    "print(\"remaining:\", remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the complete reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructRules(ReconstructRules):\n",
    "    def reconstruct_key(self, refined_key, log=False):\n",
    "        keys = [refined_key]\n",
    "        defined = set()\n",
    "        while keys:\n",
    "            if log: print(len(keys))\n",
    "            key_to_reconstruct, *keys = keys\n",
    "            if log: print('reconstructing:', key_to_reconstruct)\n",
    "            if key_to_reconstruct in defined:\n",
    "                raise Exception('Key found:', key_to_reconstruct)\n",
    "            defined.add(key_to_reconstruct)\n",
    "            bexpr = BExpr(refinement(key_to_reconstruct))\n",
    "            nrek = normalize(key_to_reconstruct)\n",
    "            if bexpr.simple():\n",
    "                nkey = bexpr.with_key(key_to_reconstruct)\n",
    "                if log: print('simplified_to:', nkey)\n",
    "                d, s = self.reconstruct_rules_from_bexpr(nrek, bexpr)\n",
    "                self.grammar = {**self.grammar, **{key_to_reconstruct:d}}\n",
    "            else:\n",
    "                nkey = nrek # base key\n",
    "            keys = undefined_keys(self.grammar)\n",
    "        return self.grammar, refined_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(grammar, start, log=False):\n",
    "    rr = ReconstructRules(grammar)\n",
    "    grammar, start = utils.grammar_gc(*rr.reconstruct_key(start, log))\n",
    "    return grammar, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar ={**EXPR_DZERO_G, **EXPR_DPAREN_G}\n",
    "g_, s_ = complete(grammar, '<start and(D1,Z1)>')\n",
    "gf = fuzzer.LimitFuzzer(g_)\n",
    "for i in range(10):\n",
    "    v_ = gf.iter_gen_key(key=s_, max_depth=10)\n",
    "    v = utils.tree_to_str(v_)\n",
    "    assert expr_div_by_zero(v) and expr_double_paren(v)\n",
    "    print(v)\n",
    "\n",
    "grammar ={**EXPR_DZERO_G, **EXPR_DPAREN_G}\n",
    "g_, s_ = complete(grammar, '<start or(D1,Z1)>')\n",
    "gf = fuzzer.LimitFuzzer(g_)\n",
    "for i in range(10):\n",
    "    v_ = gf.iter_gen_key(key=s_, max_depth=10)\n",
    "    v = utils.tree_to_str(v_)\n",
    "    assert expr_div_by_zero(v) or hdd.expr_double_paren(v)\n",
    "    print(v)\n",
    "    if expr_div_by_zero(v) == hdd.PRes.success: print('>', 1)\n",
    "    if expr_double_paren(v) == hdd.PRes.success: print('>',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A grammar with no fault inducing fragments.\n",
    "\n",
    "We saw how to insert a single evocative pattern into a grammar.\n",
    "A similar procedure can be used to make sure that no evocative\n",
    "fragments are present in inputs generated by given grammar.\n",
    "The idea is as follows.\n",
    "\n",
    "## Unreachable Grammar\n",
    "\n",
    "We start with the `get_reachable_positions()` output. If we can ensure\n",
    "that no nonterminals in the reachable_positions can actually produce a fault\n",
    "inducing fragment, then we are done. So, given the `get_reachable_positions`\n",
    "we can produce the unreachable grammar.\n",
    "\n",
    "For ease of discussion, we name a\n",
    "nonterminal E that is guaranteed to not produce fault tree `F` as `<E neg(F)>`.\n",
    "That is, a tree that starts from <start neg(F)> is guaranteed not to contain\n",
    "the fault tree `F`.\n",
    "\n",
    "So, the definition of `<E neg(F)` is simple enough given the characterizing\n",
    "node of the fault tree, and the corresponding reaching positions in the\n",
    "grammar.\n",
    "For each expansion rule of `<E>`, we have to make sure that it does not lead\n",
    "to `F`. So rules for `<E>` that did not have reachable positions corresponding\n",
    "to characterizing node of `F` can be directly added to `<E neg(F)>`. Next,\n",
    "for any rule that contained reachable positions, for all such positions, we\n",
    "specialize the nonterminal in that position by `neg(F)`. This gives us the\n",
    "unreachable grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_suffix(fault):\n",
    "    assert fault\n",
    "    return 'neg(%s)' % fault\n",
    "\n",
    "def unreachable_key(grammar, key, cnodesym, negated_suffix, reachable):\n",
    "    rules = grammar[key]\n",
    "    my_rules = []\n",
    "    for rule in grammar[key]:\n",
    "        positions = abstractinginputs.get_reachable_positions(rule, cnodesym, reachable)\n",
    "        if not positions:\n",
    "            # not embeddable here. We can add this rule.\n",
    "            my_rules.append(rule)\n",
    "        else:\n",
    "            new_rule = [refine_base_key(t, negated_suffix)\n",
    "                    if p in positions else t for p,t in enumerate(rule)]\n",
    "            my_rules.append(new_rule)\n",
    "    return (refine_base_key(key, negated_suffix), my_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaching = abstractinginputs.reachable_dict(EXPR_GRAMMAR)\n",
    "for key in EXPR_GRAMMAR:\n",
    "    fk, rules = unreachable_key(EXPR_GRAMMAR, key, '<factor>',\n",
    "                                negate_suffix('F1'), reaching)\n",
    "    print(fk)\n",
    "    for r in rules:\n",
    "        print('    ', r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define unreachable grammar using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unreachable_grammar(grammar, start, cnodesym, negated_suffix, reachable):\n",
    "    new_grammar = {}\n",
    "    s_key = None\n",
    "    for key in grammar:\n",
    "        fk, rules = unreachable_key(grammar, key, cnodesym, negated_suffix, reachable)\n",
    "        assert fk not in new_grammar\n",
    "        if key == start: s_key = fk\n",
    "        new_grammar[fk] = rules\n",
    "    return new_grammar, s_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negated pattern grammar.\n",
    "\n",
    "For negated pattern grammars, there are two parts. The first part is for\n",
    "pattern rules. The idea is to make sure that we can produce any but not the\n",
    "specific pattern in the current expansion. Next, we also need to make sure\n",
    "that the original fault is not reachable from any of the nonterminals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_nonterminal(k):\n",
    "    return '<%s %s>' % (stem(k), negate_suffix(refinement(k)))\n",
    "\n",
    "def rule_normalized_difference(rulesA, rulesB):\n",
    "    rem_rulesA = rulesA\n",
    "    for ruleB in rulesB:\n",
    "        rem_rulesA = [rA for rA in rem_rulesA\n",
    "                if not normalized_rule_match(rA, ruleB)]\n",
    "    return rem_rulesA\n",
    "\n",
    "def unmatch_a_refined_rule_in_pattern_grammar(refined_rule):\n",
    "    negated_rules = []\n",
    "    for pos,token in enumerate(refined_rule):\n",
    "        if not utils.is_nt(token): continue\n",
    "        if is_base_key(token): continue\n",
    "        r = [negate_nonterminal(t) if i==pos else\n",
    "                 (normalize(t) if utils.is_nt(t) else t)\n",
    "                 for i,t in enumerate(refined_rule)]\n",
    "        negated_rules.append(r)\n",
    "    return negated_rules\n",
    "\n",
    "def unmatch_definition_in_pattern_grammar(refined_rules, base_rules):\n",
    "    # Given the set of rules, we take one rule at a time,\n",
    "    # and generate the negated rule set from that.\n",
    "    negated_rules_refined = []\n",
    "    for ruleR in refined_rules:\n",
    "        neg_rules = unmatch_a_refined_rule_in_pattern_grammar(ruleR)\n",
    "        negated_rules_refined.extend(neg_rules)\n",
    "\n",
    "    # Finally, we need to add the other non-matching rules to the pattern def.\n",
    "    negated_rules_base = rule_normalized_difference(base_rules, refined_rules)\n",
    "\n",
    "    return negated_rules_refined + negated_rules_base\n",
    "\n",
    "\n",
    "def unmatch_pattern_grammar(pattern_grammar, pattern_start, base_grammar):\n",
    "    negated_grammar = {}\n",
    "    for l_key in pattern_grammar:\n",
    "        l_rule = pattern_grammar[l_key][0]\n",
    "        nl_key = negate_nonterminal(l_key)\n",
    "        # find all rules that do not match, and add to negated_grammar,\n",
    "        normal_l_key = normalize(l_key)\n",
    "        base_rules = base_grammar[normal_l_key]\n",
    "        refined_rules = pattern_grammar[l_key]\n",
    "\n",
    "        negated_rules = unmatch_definition_in_pattern_grammar(refined_rules,\n",
    "                                                              base_rules)\n",
    "        negated_grammar[nl_key] = negated_rules\n",
    "    return {**negated_grammar, **pattern_grammar} , negate_nonterminal(pattern_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETREE_DPAREN = ('<factor>',\n",
    "        [('(', [], {'abstract': False}),\n",
    "         ('<expr>',\n",
    "             [('<term>',\n",
    "                 [('<factor>',\n",
    "                     [('(', [], {'abstract': False}),\n",
    "                      ('<expr>', [('<term>',\n",
    "                          [('<factor>',\n",
    "                              [('<integer>',\n",
    "                                  [('<digit>',\n",
    "                                      [('4', [],\n",
    "                                          {'abstract': False})],\n",
    "                                      {'abstract': False})],\n",
    "                                  {'abstract': False})],\n",
    "                              {'abstract': False})],\n",
    "                          {'abstract': False})],\n",
    "                          {'abstract': True}),\n",
    "                      (')', [],\n",
    "                          {'abstract': False})],\n",
    "                      {'abstract': False})],\n",
    "                 {'abstract': False})],\n",
    "             {'abstract': False}),\n",
    "         (')', [], {'abstract': False})], {'abstract': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractinginputs.abstract_tree_to_str(ETREE_DPAREN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETREE_DZERO = ('<term>',\n",
    " [('<factor>',\n",
    "   [('<integer>',\n",
    "     [('<digit>', [('2', [], {'abstract': False})], {'abstract': False})],\n",
    "     {'abstract': False})],\n",
    "   {'abstract': True}),\n",
    "  ('/', [], {'abstract': False}),\n",
    "  ('<term>',\n",
    "   [('<factor>',\n",
    "     [('<integer>',\n",
    "       [('<digit>', [('0', [], {'abstract': False})], {'abstract': False})],\n",
    "       {'abstract': False})],\n",
    "     {'abstract': False})],\n",
    "   {'abstract': False})],\n",
    " {'abstract': False}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractinginputs.abstract_tree_to_str(ETREE_DZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_g,pattern_s, t = abstractinginputs.pattern_grammar(ETREE_DPAREN, 'F1')\n",
    "nomatch_g, nomatch_s = unmatch_pattern_grammar(pattern_g,\n",
    "                                                   pattern_s, EXPR_GRAMMAR)\n",
    "utils.display_grammar(nomatch_g, nomatch_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for negated pattern grammars, not only do we need to make sure that the\n",
    "pattern is not directly matchable, but also that the pattern cannot be\n",
    "embedded. For that we simply conjunct it with `neg(F1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_suffix(k1, suffix):\n",
    "    if is_base_key(k1):\n",
    "        return '<%s %s>' % (stem(k1), suffix)\n",
    "    return '<%s and(%s,%s)>' % (stem(k1), refinement(k1), suffix)\n",
    "\n",
    "def base_rep(t):\n",
    "    if utils.is_nt(t):\n",
    "        return normalize(t)\n",
    "    return t\n",
    "\n",
    "def negate_pattern_grammar(pattern_grammar, pattern_start, base_grammar,\n",
    "        nfault_suffix):\n",
    "    reachable_keys = abstractinginputs.reachable_dict(base_grammar)\n",
    "    nomatch_g, nomatch_s = unmatch_pattern_grammar(pattern_grammar,\n",
    "                                                   pattern_start, base_grammar)\n",
    "\n",
    "    new_grammar = {}\n",
    "\n",
    "    my_key = normalize(pattern_start)\n",
    "    # which keys can reach pattern_start?\n",
    "    keys_that_can_reach_fault = [k for k in reachable_keys\n",
    "                                if my_key in reachable_keys[k]]\n",
    "    #for k in keys_that_can_reach_fault: assert my_key in reachable_keys[k]\n",
    "    new_g = {}\n",
    "    for k in nomatch_g: \n",
    "        new_rules = []\n",
    "        for rule in nomatch_g[k]:\n",
    "            new_rule = [and_suffix(t, nfault_suffix)\n",
    "                        if base_rep(t) in keys_that_can_reach_fault\n",
    "                        else t for t in rule]\n",
    "            new_rules.append(new_rule)\n",
    "        new_g[k] = new_rules\n",
    "    return new_g, negate_nonterminal(pattern_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomatch_g, nomatch_s = negate_pattern_grammar(pattern_g, pattern_s,\n",
    "                                            EXPR_GRAMMAR, 'neg(F1)')\n",
    "# next we need to conjunct\n",
    "utils.display_grammar(nomatch_g, nomatch_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can now define our `negated_grammar()`\n",
    "The new grammar is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_fault_grammar(grammar, start_symbol, cnode, fname):\n",
    "    key_f = cnode[0]\n",
    "    pattern_g, pattern_s, tr = abstractinginputs.pattern_grammar(cnode, fname)\n",
    "    negated_suffix = negate_suffix(fname)\n",
    "    nomatch_g, nomatch_s = negate_pattern_grammar(pattern_g,\n",
    "                                pattern_s, grammar, negated_suffix)\n",
    "\n",
    "    reachable_keys = abstractinginputs.reachable_dict(grammar)\n",
    "    reach_g, reach_s = abstractinginputs.reachable_grammar(grammar,\n",
    "                                start_symbol, key_f, fname, reachable_keys)\n",
    "    unreach_g, unreach_s = unreachable_grammar(grammar,\n",
    "                                start_symbol, key_f, negated_suffix, reachable_keys)\n",
    "\n",
    "    combined_grammar = {**grammar, **nomatch_g, **reach_g, **unreach_g}\n",
    "    unreaching_sym = refine_base_key(key_f, negated_suffix)\n",
    "\n",
    "    # We cant add `unreach_g[unreaching_sym]` directly to\n",
    "    # `combined_grammar[unreaching_sym]` because it will then match\n",
    "    # ```\n",
    "    # [['<factor neg(F1)>',\n",
    "    #         [['(', []],\n",
    "    #          ['<expr neg(F1)>',\n",
    "    #              [['<term neg(F1)>',\n",
    "    #                  [['<factor neg(F1)>',\n",
    "    #                      [['(', []],\n",
    "    #                       ['<expr neg(F1)>', ],\n",
    "    #                       [')', []]]]]]]],\n",
    "    #          [')',    []]]]]\n",
    "    # ```\n",
    "    # So, what we will do, is to make sure that the combined rules do not either\n",
    "    # reach the negated patterns nor do the match the negated patterns.\n",
    "\n",
    "    anded_defs = and_definitions(unreach_g[unreaching_sym],\n",
    "                                            nomatch_g[nomatch_s])\n",
    "\n",
    "    combined_grammar[unreaching_sym] = anded_defs\n",
    "    \n",
    "    return combined_grammar, unreach_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnode = ETREE_DPAREN\n",
    "g, s = utils.grammar_gc(*no_fault_grammar(EXPR_GRAMMAR, EXPR_START, cnode, 'F1'))\n",
    "utils.display_grammar(g, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grammar is now guaranteed not to produce any instance of the characterizing node.\n",
    "However, as you can see the grammar is not complete. For completing the\n",
    "grammar We need to rely on `reconstruction` that we discussed in the last post.\n",
    "\n",
    "Aside: Let us construct another function that checks the double\n",
    "parenthesis we abstracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def check_doubled_paren(val):\n",
    "    while '((' in val and '))' in val:\n",
    "        val = re.sub(r'[^()]+','X', val)\n",
    "        if '((X))' in val:\n",
    "            return hdd.PRes.success\n",
    "        val = val.replace(r'(X)', '')\n",
    "    return hdd.PRes.failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert check_doubled_paren('((1))') == hdd.PRes.success\n",
    "assert check_doubled_paren('((1)+(2))') == hdd.PRes.failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation of full evocative expressions\n",
    "\n",
    "Negation of a single pattern is useful, but we may also want\n",
    "to negate larger expressions such as say `neg(or(and(f1,f2),f3))`. However, we\n",
    "do not need to implement the complete negation as before. Instead, we can rely\n",
    "on the fact that our evocative expressions are simply boolean expressions.\n",
    "\n",
    "That is, expressions such `neg(or(A,B))` can be simplified as\n",
    "`and(neg(A),neg(B))` and `neg(and(A,B))` can be simplified as\n",
    "`or(neg(A),neg(B))`. This means that we do not need to implement the negation\n",
    "beyond negating simple faults.\n",
    "\n",
    "Here is an example of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnode = ETREE_DPAREN\n",
    "g1, s1 = utils.grammar_gc(*no_fault_grammar(EXPR_GRAMMAR, EXPR_START, ETREE_DPAREN, 'D1'))\n",
    "g2, s2 = utils.grammar_gc(*no_fault_grammar(EXPR_GRAMMAR, EXPR_START, ETREE_DZERO, 'Z1'))\n",
    "grammar ={**EXPR_GRAMMAR, **g1,**g2}\n",
    "g_, s_ = complete(grammar, '<start neg(or(D1,Z1))>')\n",
    "utils.display_grammar(g_,s_)\n",
    "print()\n",
    "gf = fuzzer.LimitFuzzer(g_)\n",
    "for i in range(100):\n",
    "    t = gf.iter_gen_key(key=s_, max_depth=10)\n",
    "    v = utils.tree_to_str(t)\n",
    "    assert expr_div_by_zero(v) == hdd.PRes.failed and check_doubled_paren(v) == hdd.PRes.failed, (v, t)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    g_, s_ = complete(grammar, '<start neg(and(D1,Z1))>')\n",
    "    utils.display_grammar(g_,s_)\n",
    "    print()\n",
    "    gf = fuzzer.LimitFuzzer(g_)\n",
    "    for i in range(100):\n",
    "        t = gf.iter_gen_key(key=s_, max_depth=10)\n",
    "        v = utils.tree_to_str(t)\n",
    "        assert expr_div_by_zero(v) == hdd.PRes.failed or check_doubled_paren(v) == hdd.PRes.failed, (v, t)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
